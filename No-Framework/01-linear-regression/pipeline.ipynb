{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81c96125",
   "metadata": {},
   "source": [
    "# No-Framework Linear Regression\n",
    "\n",
    "### This implementation uses only Numpy to build a linear regression model\n",
    "\n",
    "### Goal: Predict used car prices using gradient descent optimization\n",
    "\n",
    "What we'll implement manually:\n",
    "- Train/Test split\n",
    "- Feature scaling (z-score normalization)\n",
    "- Forward pass (predictions)\n",
    "- Cost function (Mean Squared Error)\n",
    "- Gradient computation\n",
    "- Parameter updates (gradient descent)\n",
    "- Evaluation metrics (MSE, RMSE, R^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35ea3236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy: Core library for numerical operations on arrays\n",
    "# ONLY external dependency for the model itself\n",
    "import numpy as np\n",
    "\n",
    "# matplotlib: for creating visualizations of training progress and results\n",
    "import matplotlib as plt\n",
    "\n",
    "# os: for handling file paths in a cross-platform way\n",
    "import os\n",
    "\n",
    "# Set random seed for reporducibility\n",
    "# Project-wide seed of 113\n",
    "np.random.seed(113)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9f7dd7",
   "metadata": {},
   "source": [
    "# Load cleaned data\n",
    "\n",
    "- Load the pre-processed dataset that was cleaned in the data-preperation step\n",
    "- This same file will be used by all 4 frameworks for fair comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72c01dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (100000, 12)\n",
      "First row: [2.9990e+04 2.0140e+03 7.0000e+00 2.0000e+00 6.0000e+00 2.0000e+00\n",
      " 2.6129e+04 0.0000e+00 2.0000e+00 0.0000e+00 8.0000e+00 1.7000e+01]\n"
     ]
    }
   ],
   "source": [
    "# Define path to our cleaned dataset\n",
    "DATA_PATH = os.path.join('..', '..', 'data', 'processed', 'vehicles_clean.csv')\n",
    "\n",
    "# np.genfromtxt() reads CSV files into numpy arrays\n",
    "# delimiter=',' specifies that columns are seperated by comas\n",
    "# skip_header=1 skips the first row (column names)\n",
    "# Gives us a 2D array where each row is a car, each column is a feature\n",
    "data = np.genfromtxt(DATA_PATH, delimiter=',', skip_header=1)\n",
    "\n",
    "# Verify the data loaded correctly\n",
    "# shape should be (100000, 12)\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"First row: {data[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf166fd",
   "metadata": {},
   "source": [
    "# Seperate features and Target\n",
    "- Our columns are: price, year, manufacturer, condition, cylinders, fuel, odometer, title_status, transmission, drive, type, state\n",
    "- price (column 0) is our TARGET - what we want to predict\n",
    "- All other columns (1-11) are FEATURES - inputs to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58f9c290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features (X) shape: (100000, 11)\n",
      "Target (y) shape: (100000,)\n"
     ]
    }
   ],
   "source": [
    "# Extract target variable (price)\n",
    "# data[:, 0] means \"all rows, column 0\"\n",
    "y = data[:, 0]\n",
    "\n",
    "# Extract features variables\n",
    "# data[:, 1:] means \"all rows, column 1 through the end\"\n",
    "X = data[:, 1:]\n",
    "\n",
    "# Print shapes to verify seperation\n",
    "print(f\"Features (X) shape: {X.shape}\")\n",
    "print(f\"Target (y) shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e66e1a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names: ['year', 'manufacturer', 'condition', 'cylinders', 'fuel', 'odometer', 'title_status', 'transmission', 'drive', 'type', 'state']\n"
     ]
    }
   ],
   "source": [
    "# Define feature names for reference (matching our cleaned data columns)\n",
    "FEATURE_NAMES = ['year', 'manufacturer', 'condition', 'cylinders', 'fuel', 'odometer', 'title_status', 'transmission', 'drive', 'type', 'state']\n",
    "print(f\"Feature Names: {FEATURE_NAMES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ea609e",
   "metadata": {},
   "source": [
    "# Train/Test Split\n",
    "\n",
    "We need to split our data into two sets:\n",
    "- Training set (80%): Used to train the model (learn the weights)\n",
    "- Trest set (20%): Used to evaluate performance on unseen data\n",
    "\n",
    "Why Split? If we test on the same data we traine don, we can't tell if the model actually learned patterns or just memorized the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c4d8eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 80,000 samples (80%)\n",
      "Test set size: 20,000 samples (20%)\n",
      "\n",
      "X_train shape: (80000, 11)\n",
      "X_test shape: (20000, 11)\n",
      "y_train shape: (80000,)\n",
      "y_test shape: (20000,)\n"
     ]
    }
   ],
   "source": [
    "def train_test_split(X, y, test_size=0.2, random_seed=113):\n",
    "    \"\"\"\n",
    "    Split features and target into training and testing sets.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Feature matrix of shape (n_samples, n_features)\n",
    "\n",
    "    y : numpy.ndarray\n",
    "        Target vector of shape (n_samples,)\n",
    "    \n",
    "    test_size : float\n",
    "        Proportion of data to use for testing (0.0 to 1.0)\n",
    "        Default 0.2 means 20% test, 80% train\n",
    "    \n",
    "    random_seed : int\n",
    "        Seed for random number generator to ensure reproducibility\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X_train, X_test, y_train, y_test : numpy.ndarray\n",
    "        Split arrays for training and testing\n",
    "    \"\"\"\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Get the total number of samples in our dataset\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    # Calculate the number of test samples\n",
    "    # int() truncates to whole number\n",
    "    n_test = int(n_samples * test_size)\n",
    "\n",
    "    # Calculate number of training samples\n",
    "    n_train = n_samples - n_test\n",
    "\n",
    "    # Create an array of all indices\n",
    "    indices = np.arange(n_samples)\n",
    "\n",
    "    # Randomly shuffle the indices\n",
    "    # np.random.shuffle() modifies the array in place\n",
    "    # This randomizes which samples go to train vs test\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Split indices into train and test portions\n",
    "    # First n_train indices go to training set\n",
    "    train_indices = indices[:n_train]\n",
    "    # Remaining indices go to test set\n",
    "    test_indices = indices[n_train:]\n",
    "\n",
    "    # Use the indices to select rows from X and y\n",
    "    # X[train_indices] selects rows at those index positions\n",
    "    X_train = X[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_train = y[train_indices]\n",
    "    y_test = y[test_indices]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# Perform the split using our funciton\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_seed=113)\n",
    "\n",
    "# Verify the split worked correctly\n",
    "print(f\"Training set size: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.0f}%)\")\n",
    "print(f\"Test set size: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.0f}%)\")\n",
    "print(f\"\\nX_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9a458c",
   "metadata": {},
   "source": [
    "# Feature Scaling (Z-score normalization)\n",
    "\n",
    "Feature scaling is critical for gradient descent to work properly.\n",
    "\n",
    "Why scale features?\n",
    "Our features have very different ranges:\n",
    "- year: 1990-2022 (range of 32)\n",
    "- odometer: 100-500,000 (range of 500,000)\n",
    "- manufacturer: 0-40 (range of 40)\n",
    "\n",
    "Without scaling, features with large values dominate the gradients, causing gradient descent to zigzag inefficently or fail to converge.\n",
    "\n",
    "Z-score normalization transforms each feature to have\n",
    "- Mean = 0\n",
    "- Standard deviation = 1\n",
    "Formula: x_scaled = (x - mean) / std\n",
    "\n",
    "IMPORTANT: We calculate mean and std from TRAINING data only!\n",
    "Using test data statistics would be \"data leakage\" - the model would indirectly learn information about the test set during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c6de622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling parameters (computed from training data:)\n",
      "\n",
      "Feature                    Mean Std\n",
      "-----------------------------------------------\n",
      "year                    2012.32            5.78\n",
      "manufacturer              18.20           11.47\n",
      "condition                  3.09            2.44\n",
      "cylinders                  6.00            1.92\n",
      "fuel                       2.05            0.78\n",
      "odometer               94285.02        63064.30\n",
      "title_status               0.24            1.06\n",
      "transmission               0.39            0.77\n",
      "drive                      1.41            1.21\n",
      "type                       7.15            4.12\n",
      "state                     23.64           15.10\n",
      "\n",
      "--- Verification (Training Data After Scaling) ---\n",
      "Mean of each feature (should be =0): [-0. -0. -0. -0.  0.  0.  0. -0. -0. -0.  0.]\n",
      "Std of each feature (should be 1): [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "def compute_scaling_params(X_train):\n",
    "    \"\"\"\n",
    "    Compute mean and standard deviation for each feature from training data.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : numpy.ndarray\n",
    "        Training feature matrix of shape (n_samples, n_features)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    means: numpy.ndarray\n",
    "        Mean of each feature, shape (n_features,)\n",
    "    stds : numpy.ndarray\n",
    "        Standard deviation of each feature, shape (n_features,)\n",
    "    \"\"\"\n",
    "    # np.mean() with axis=0 computes mean for each column (feature)\n",
    "    # Results in a 1D array with one mean value per feature\n",
    "    means = np.mean(X_train, axis=0)\n",
    "\n",
    "    # np.std() with axis=0 computes standard deviation for each column\n",
    "    # Result is a 1D array with one std value per feature\n",
    "    stds = np.std(X_train, axis=0)\n",
    "\n",
    "    return means, stds\n",
    "\n",
    "def scale_features(X, means, stds):\n",
    "    \"\"\"\n",
    "    Apply z-score normalization to features.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Feature matrix to scale, shape (n_samples, n_features)\n",
    "    means : numpy.ndarray\n",
    "        Mean of each feature (from training data)\n",
    "    stds : numpy.ndarray\n",
    "        Standard deviation of each feature (from training data)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X_scaled : numpy.ndarray\n",
    "        Noramlized feature matrix with mean=0, std=1 for each feature\n",
    "    \"\"\"\n",
    "    # Apply z-score formula: (X - mean) / stds\n",
    "    # Numpy broadcasting handles the element-wise operations automatically\n",
    "    # Each column is subtracted by its mean, then divided by its std\n",
    "    X_scaled = (X - means) / stds\n",
    "\n",
    "    return X_scaled\n",
    "\n",
    "# Step 1: Compute scaling parameters from TRAINING data only\n",
    "means, stds = compute_scaling_params(X_train)\n",
    "\n",
    "# Display the computed parameters for each feature\n",
    "print(\"Scaling parameters (computed from training data:)\\n\")\n",
    "print(f\"{'Feature':<15} {'Mean':>15} {'Std':.15}\")\n",
    "print(\"-\" *47)\n",
    "for i, name in enumerate(FEATURE_NAMES):\n",
    "    print(f\"{name:<15} {means[i]:>15.2f} {stds[i]:>15.2f}\")\n",
    "\n",
    "# Step 2: Apply scaling to both training and test data\n",
    "# IMPORTANT: Use the same means and stds (from training) for both sets\n",
    "X_train_scaled = scale_features(X_train, means, stds)\n",
    "X_test_scaled = scale_features(X_test, means, stds)\n",
    "\n",
    "# Verify scaling worked - training data should have mean=0 and std=1\n",
    "print(\"\\n--- Verification (Training Data After Scaling) ---\")\n",
    "print(f\"Mean of each feature (should be =0): {np.mean(X_train_scaled, axis=0).round(6)}\")\n",
    "print(f\"Std of each feature (should be 1): {np.std(X_train_scaled, axis=0).round(6)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05134ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML Portfolio (.venv)",
   "language": "python",
   "name": "ml-portfolio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
