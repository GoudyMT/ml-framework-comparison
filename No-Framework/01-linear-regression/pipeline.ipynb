{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81c96125",
   "metadata": {},
   "source": [
    "# No-Framework Linear Regression\n",
    "\n",
    "### This implementation uses only Numpy to build a linear regression model\n",
    "\n",
    "### Goal: Predict used car prices using gradient descent optimization\n",
    "\n",
    "What we'll implement manually:\n",
    "- Train/Test split\n",
    "- Feature scaling (z-score normalization)\n",
    "- Forward pass (predictions)\n",
    "- Cost function (Mean Squared Error)\n",
    "- Gradient computation\n",
    "- Parameter updates (gradient descent)\n",
    "- Evaluation metrics (MSE, RMSE, R^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35ea3236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy: Core library for numerical operations on arrays\n",
    "# ONLY external dependency for the model itself\n",
    "import numpy as np\n",
    "\n",
    "# matplotlib: for creating visualizations of training progress and results\n",
    "import matplotlib as plt\n",
    "\n",
    "# os: for handling file paths in a cross-platform way\n",
    "import os\n",
    "\n",
    "# Set random seed for reporducibility\n",
    "# Project-wide seed of 113\n",
    "np.random.seed(113)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9f7dd7",
   "metadata": {},
   "source": [
    "# Load cleaned data\n",
    "\n",
    "- Load the pre-processed dataset that was cleaned in the data-preperation step\n",
    "- This same file will be used by all 4 frameworks for fair comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72c01dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (100000, 12)\n",
      "First row: [2.9990e+04 2.0140e+03 7.0000e+00 2.0000e+00 6.0000e+00 2.0000e+00\n",
      " 2.6129e+04 0.0000e+00 2.0000e+00 0.0000e+00 8.0000e+00 1.7000e+01]\n"
     ]
    }
   ],
   "source": [
    "# Define path to our cleaned dataset\n",
    "DATA_PATH = os.path.join('..', '..', 'data', 'processed', 'vehicles_clean.csv')\n",
    "\n",
    "# np.genfromtxt() reads CSV files into numpy arrays\n",
    "# delimiter=',' specifies that columns are seperated by comas\n",
    "# skip_header=1 skips the first row (column names)\n",
    "# Gives us a 2D array where each row is a car, each column is a feature\n",
    "data = np.genfromtxt(DATA_PATH, delimiter=',', skip_header=1)\n",
    "\n",
    "# Verify the data loaded correctly\n",
    "# shape should be (100000, 12)\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"First row: {data[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf166fd",
   "metadata": {},
   "source": [
    "# Seperate features and Target\n",
    "- Our columns are: price, year, manufacturer, condition, cylinders, fuel, odometer, title_status, transmission, drive, type, state\n",
    "- price (column 0) is our TARGET - what we want to predict\n",
    "- All other columns (1-11) are FEATURES - inputs to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58f9c290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features (X) shape: (100000, 11)\n",
      "Target (y) shape: (100000,)\n"
     ]
    }
   ],
   "source": [
    "# Extract target variable (price)\n",
    "# data[:, 0] means \"all rows, column 0\"\n",
    "y = data[:, 0]\n",
    "\n",
    "# Extract features variables\n",
    "# data[:, 1:] means \"all rows, column 1 through the end\"\n",
    "X = data[:, 1:]\n",
    "\n",
    "# Print shapes to verify seperation\n",
    "print(f\"Features (X) shape: {X.shape}\")\n",
    "print(f\"Target (y) shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e66e1a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names: ['year', 'manufacturer', 'condition', 'cylinders', 'fuel', 'odometer', 'title_status', 'transmission', 'drive', 'type', 'state']\n"
     ]
    }
   ],
   "source": [
    "# Define feature names for reference (matching our cleaned data columns)\n",
    "FEATURE_NAMES = ['year', 'manufacturer', 'condition', 'cylinders', 'fuel', 'odometer', 'title_status', 'transmission', 'drive', 'type', 'state']\n",
    "print(f\"Feature Names: {FEATURE_NAMES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ea609e",
   "metadata": {},
   "source": [
    "# Train/Test Split\n",
    "\n",
    "We need to split our data into two sets:\n",
    "- Training set (80%): Used to train the model (learn the weights)\n",
    "- Trest set (20%): Used to evaluate performance on unseen data\n",
    "\n",
    "Why Split? If we test on the same data we traine don, we can't tell if the model actually learned patterns or just memorized the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c4d8eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 80,000 samples (80%)\n",
      "Test set size: 20,000 samples (20%)\n",
      "\n",
      "X_train shape: (80000, 11)\n",
      "X_test shape: (20000, 11)\n",
      "y_train shape: (80000,)\n",
      "y_test shape: (20000,)\n"
     ]
    }
   ],
   "source": [
    "def train_test_split(X, y, test_size=0.2, random_seed=113):\n",
    "    \"\"\"\n",
    "    Split features and target into training and testing sets.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Feature matrix of shape (n_samples, n_features)\n",
    "\n",
    "    y : numpy.ndarray\n",
    "        Target vector of shape (n_samples,)\n",
    "    \n",
    "    test_size : float\n",
    "        Proportion of data to use for testing (0.0 to 1.0)\n",
    "        Default 0.2 means 20% test, 80% train\n",
    "    \n",
    "    random_seed : int\n",
    "        Seed for random number generator to ensure reproducibility\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X_train, X_test, y_train, y_test : numpy.ndarray\n",
    "        Split arrays for training and testing\n",
    "    \"\"\"\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Get the total number of samples in our dataset\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    # Calculate the number of test samples\n",
    "    # int() truncates to whole number\n",
    "    n_test = int(n_samples * test_size)\n",
    "\n",
    "    # Calculate number of training samples\n",
    "    n_train = n_samples - n_test\n",
    "\n",
    "    # Create an array of all indices\n",
    "    indices = np.arange(n_samples)\n",
    "\n",
    "    # Randomly shuffle the indices\n",
    "    # np.random.shuffle() modifies the array in place\n",
    "    # This randomizes which samples go to train vs test\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Split indices into train and test portions\n",
    "    # First n_train indices go to training set\n",
    "    train_indices = indices[:n_train]\n",
    "    # Remaining indices go to test set\n",
    "    test_indices = indices[n_train:]\n",
    "\n",
    "    # Use the indices to select rows from X and y\n",
    "    # X[train_indices] selects rows at those index positions\n",
    "    X_train = X[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_train = y[train_indices]\n",
    "    y_test = y[test_indices]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# Perform the split using our funciton\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_seed=113)\n",
    "\n",
    "# Verify the split worked correctly\n",
    "print(f\"Training set size: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.0f}%)\")\n",
    "print(f\"Test set size: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.0f}%)\")\n",
    "print(f\"\\nX_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9a458c",
   "metadata": {},
   "source": [
    "# Feature Scaling (Z-score normalization)\n",
    "\n",
    "Feature scaling is critical for gradient descent to work properly.\n",
    "\n",
    "Why scale features?\n",
    "Our features have very different ranges:\n",
    "- year: 1990-2022 (range of 32)\n",
    "- odometer: 100-500,000 (range of 500,000)\n",
    "- manufacturer: 0-40 (range of 40)\n",
    "\n",
    "Without scaling, features with large values dominate the gradients, causing gradient descent to zigzag inefficently or fail to converge.\n",
    "\n",
    "Z-score normalization transforms each feature to have\n",
    "- Mean = 0\n",
    "- Standard deviation = 1\n",
    "Formula: x_scaled = (x - mean) / std\n",
    "\n",
    "IMPORTANT: We calculate mean and std from TRAINING data only!\n",
    "Using test data statistics would be \"data leakage\" - the model would indirectly learn information about the test set during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c6de622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling parameters (computed from training data:)\n",
      "\n",
      "Feature                    Mean Std\n",
      "-----------------------------------------------\n",
      "year                    2012.32            5.78\n",
      "manufacturer              18.20           11.47\n",
      "condition                  3.09            2.44\n",
      "cylinders                  6.00            1.92\n",
      "fuel                       2.05            0.78\n",
      "odometer               94285.02        63064.30\n",
      "title_status               0.24            1.06\n",
      "transmission               0.39            0.77\n",
      "drive                      1.41            1.21\n",
      "type                       7.15            4.12\n",
      "state                     23.64           15.10\n",
      "\n",
      "--- Verification (Training Data After Scaling) ---\n",
      "Mean of each feature (should be =0): [-0. -0. -0. -0.  0.  0.  0. -0. -0. -0.  0.]\n",
      "Std of each feature (should be 1): [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "def compute_scaling_params(X_train):\n",
    "    \"\"\"\n",
    "    Compute mean and standard deviation for each feature from training data.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : numpy.ndarray\n",
    "        Training feature matrix of shape (n_samples, n_features)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    means: numpy.ndarray\n",
    "        Mean of each feature, shape (n_features,)\n",
    "    stds : numpy.ndarray\n",
    "        Standard deviation of each feature, shape (n_features,)\n",
    "    \"\"\"\n",
    "    # np.mean() with axis=0 computes mean for each column (feature)\n",
    "    # Results in a 1D array with one mean value per feature\n",
    "    means = np.mean(X_train, axis=0)\n",
    "\n",
    "    # np.std() with axis=0 computes standard deviation for each column\n",
    "    # Result is a 1D array with one std value per feature\n",
    "    stds = np.std(X_train, axis=0)\n",
    "\n",
    "    return means, stds\n",
    "\n",
    "def scale_features(X, means, stds):\n",
    "    \"\"\"\n",
    "    Apply z-score normalization to features.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Feature matrix to scale, shape (n_samples, n_features)\n",
    "    means : numpy.ndarray\n",
    "        Mean of each feature (from training data)\n",
    "    stds : numpy.ndarray\n",
    "        Standard deviation of each feature (from training data)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X_scaled : numpy.ndarray\n",
    "        Noramlized feature matrix with mean=0, std=1 for each feature\n",
    "    \"\"\"\n",
    "    # Apply z-score formula: (X - mean) / stds\n",
    "    # Numpy broadcasting handles the element-wise operations automatically\n",
    "    # Each column is subtracted by its mean, then divided by its std\n",
    "    X_scaled = (X - means) / stds\n",
    "\n",
    "    return X_scaled\n",
    "\n",
    "# Step 1: Compute scaling parameters from TRAINING data only\n",
    "means, stds = compute_scaling_params(X_train)\n",
    "\n",
    "# Display the computed parameters for each feature\n",
    "print(\"Scaling parameters (computed from training data:)\\n\")\n",
    "print(f\"{'Feature':<15} {'Mean':>15} {'Std':.15}\")\n",
    "print(\"-\" *47)\n",
    "for i, name in enumerate(FEATURE_NAMES):\n",
    "    print(f\"{name:<15} {means[i]:>15.2f} {stds[i]:>15.2f}\")\n",
    "\n",
    "# Step 2: Apply scaling to both training and test data\n",
    "# IMPORTANT: Use the same means and stds (from training) for both sets\n",
    "X_train_scaled = scale_features(X_train, means, stds)\n",
    "X_test_scaled = scale_features(X_test, means, stds)\n",
    "\n",
    "# Verify scaling worked - training data should have mean=0 and std=1\n",
    "print(\"\\n--- Verification (Training Data After Scaling) ---\")\n",
    "print(f\"Mean of each feature (should be =0): {np.mean(X_train_scaled, axis=0).round(6)}\")\n",
    "print(f\"Std of each feature (should be 1): {np.std(X_train_scaled, axis=0).round(6)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4962a0d9",
   "metadata": {},
   "source": [
    "# Initialize model parameters\n",
    "\n",
    "Linear regression model: y_pred = X @ weights + bias\n",
    "\n",
    "We need to learn two things:\n",
    "- weights (w): One weight per feature - how much each feature affects price\n",
    "- bias (b): A constant offset (the predicted price when all features are 0)\n",
    "\n",
    "We initialize weights to zeros. This is a common starting point for linear regression. The gradient descent algorithm will adjust them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08e44c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 11\n",
      "Initial weights shape: (11,)\n",
      "Initial weights: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Initial bias: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Get the number of features from our training data\n",
    "n_features = X_train_scaled.shape[1]\n",
    "\n",
    "# Initial weights as a 1D array of zeros, one weight per feature\n",
    "# Shape: (11), for the 11 features\n",
    "weights = np.zeros(n_features)\n",
    "\n",
    "# Initial bias as a single scaler value of 0\n",
    "bias = 0.0\n",
    "\n",
    "print(f\"Number of features: {n_features}\")\n",
    "print(f\"Initial weights shape: {weights.shape}\")\n",
    "print(f\"Initial weights: {weights}\")\n",
    "print(f\"Initial bias: {bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f0bdcf",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "\n",
    "Hyperparameters are settings we choose before training (not learned)\n",
    "These control how the learning process works\n",
    "\n",
    "Learning rate (alpha): Controls how big each gradient descent step is\n",
    "- Too large: May overshoot the minimum and diverge\n",
    "- Too small: training will be very slow\n",
    "- 0.01 is a common starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "552eb0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyperparameters:\n",
      "    Learning rate: 0.01\n",
      "    Iterations: 1000\n"
     ]
    }
   ],
   "source": [
    "# Learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Number of iterations: How many times we update the weights\n",
    "# - More iterations = more chances to improve (but slower training)\n",
    "# - We'll start with 1000 and adjust if needed\n",
    "n_iterations = 1000\n",
    "\n",
    "print(f\"\\nHyperparameters:\")\n",
    "print(f\"    Learning rate: {learning_rate}\")\n",
    "print(f\"    Iterations: {n_iterations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaa0d61",
   "metadata": {},
   "source": [
    "# Forward pass (prediction function)\n",
    "\n",
    "The forward pass computes predictions given inputs and current parameters\n",
    "Linear regression formula: y_pred = X @ weights + bias\n",
    "\n",
    "X @ weights is matrix multiplication:\n",
    "- X has shape (n_samples, n_features)\n",
    "- weights has shape (n_features)\n",
    "- Result has shape (n_samples,)\n",
    "Each sample's prediction is the weighted sum of its features plus bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6d66d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Forward pass test (with zero weights):\n",
      "    Prediction shape: (80000,)\n",
      "    First 5 predictions: [0. 0. 0. 0. 0.]\n",
      "    (All zeros expected since weights are zero)\n"
     ]
    }
   ],
   "source": [
    "def forward(X, weights, bias):\n",
    "    \"\"\"\n",
    "    Compute predictions using Linear regression formula.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Feature matrix of shape (n_samples, n_features)\n",
    "    weights : numpy.ndarray\n",
    "        Weight vector of shape (n_features,)\n",
    "    bias : float\n",
    "        Bias term (scaler)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    y_pred : numpy.ndarray\n",
    "        Predicted value of shape (n_samples,)\n",
    "    \"\"\"\n",
    "    # @ is matrix multiplication operation in Python/Numpy\n",
    "    # X @ weights computes: sum of (features_i * weight_i) for each sample\n",
    "    # Adding bias shifts all predictions by a constant amount\n",
    "    y_pred = X @ weights + bias\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "# Test the forward pass with our initial (zero) weights\n",
    "# With zero weights and zero bias, all predictions should be 0\n",
    "y_pred_initial = forward(X_train_scaled, weights, bias)\n",
    "\n",
    "print(f\"\\nForward pass test (with zero weights):\")\n",
    "print(f\"    Prediction shape: {y_pred_initial.shape}\")\n",
    "print(f\"    First 5 predictions: {y_pred_initial[:5]}\")\n",
    "print(f\"    (All zeros expected since weights are zero)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7624b193",
   "metadata": {},
   "source": [
    "# Cost function (Mean Squared Error)\n",
    "\n",
    "The cost function measures how wrong our predictions are.\n",
    "We use mean squared error (MSE) - the average of squared differences.\n",
    "\n",
    "Formula: MSE = (1/2n) * Σ(y_pred - y)²\n",
    "\n",
    "Why squared?\n",
    "- Makes all errors positive (no canceling out)\n",
    "- Penalizes large errors more than small errors\n",
    "\n",
    "Why divide by 2n instead of n?\n",
    "- The 2 is a convenience for gradient calculation\n",
    "- When we take the derivative, the 2 cancels out with the exponent\n",
    "- This is a common convention in ML (doesn't change the optimal weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dbac101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Compute Mean Squared Error cost.\n",
      "\n",
      "    Parameters:\n",
      "    -----------\n",
      "    y_pred : numpy.ndarray\n",
      "        Predicted values of shapes (n_samples,)\n",
      "    y_true : numpy.ndarray\n",
      "        Actual values of shape (n_samples,)\n",
      "\n",
      "    Returns:\n",
      "    --------\n",
      "    cost : float\n",
      "        The mean squared error (scalar value)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "def compute_cost(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Compute Mean Squared Error cost.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_pred : numpy.ndarray\n",
    "        Predicted values of shapes (n_samples,)\n",
    "    y_true : numpy.ndarray\n",
    "        Actual values of shape (n_samples,)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    cost : float\n",
    "        The mean squared error (scalar value)\n",
    "    \"\"\"\n",
    "    # Get the number of samples\n",
    "    n_samples = len(y_true)\n",
    "\n",
    "    # Calculate the error (difference between prediction and actual)\n",
    "    # This is a vector of errors, one per sample\n",
    "    errors = y_pred - y_true\n",
    "\n",
    "    # Square each error, sum them all, then average\n",
    "    cost = np.sum(errors ** 2) / (2 * n_samples)\n",
    "\n",
    "    return cost\n",
    "\n",
    "print(compute_cost.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7507c59c",
   "metadata": {},
   "source": [
    "# Gradient Computation\n",
    "\n",
    "Gradients tell us how to adjust weights and bias to reduce the cost.\n",
    "\n",
    "The gradient is the partial derivative of the cost with respect to each paramter.\n",
    "It points in the direction of steepest INCREASE, so we subtract it to decrease cost.\n",
    "\n",
    "Intuition:\n",
    "- If predictions are too high (y_pred > y), gradients are positive\n",
    "- Subtracting positive gradients decreaese weights, lowering predictions\n",
    "- If predictions are too low (y_pred < y), gradients are negaative\n",
    "- Subtracting negative gradients increases weights, raising predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41e27b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial cost (with zero weights): 284793019.84\n",
      "This is high because we're predicting $0 for every car!\n",
      "\n",
      "Initial gradients:\n",
      "    dW shape: (11,)\n",
      "    dW values: [-8414.96   691.31 -1702.91 -3161.35   774.12  7961.75  1188.04 -3809.45\n",
      "  1301.62   229.63  -306.96]\n",
      "    db value: -19,195.34\n",
      "\n",
      "  (db is negative because predictions are too LOW -we need to increase bias)\n"
     ]
    }
   ],
   "source": [
    "def compute_gradients(X, y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Compute gradients of the cost with respect to weights and bias.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Feature matrix of shape (n_samples, n_features)\n",
    "    y_pred : numpy.ndarray\n",
    "        Predicted values of shape (n_samples,)\n",
    "    y_true : numpy.ndarray\n",
    "        Actual values of shape (n_samples,)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dW : numpy.ndarray\n",
    "        Gradient with respect to weights, shape (n_features,)\n",
    "    db : float\n",
    "        Gradient with respect to bias (scaler)\n",
    "    \"\"\"\n",
    "    # Get the number of samples\n",
    "    n_samples = len(y_true)\n",
    "\n",
    "    # Calculate errors (predictions minus actual values)\n",
    "    errors = y_pred - y_true\n",
    "\n",
    "    # Gradient for weights : dW = (1/n) * X.T @ errors\n",
    "    # X.T has shape (n_features, n_samples)\n",
    "    # errors has shape (n_samples,)\n",
    "    # X.T @ errors has shape (n_features,) - one gradient per weight\n",
    "    dW = (1 / n_samples) * (X.T @ errors)\n",
    "\n",
    "    # Gradient for bias: db = (1/n) * sum(errors)\n",
    "    # This is just the average error (scaler value)\n",
    "    db = (1 / n_samples) * np.sum(errors)\n",
    "\n",
    "    return dW, db\n",
    "\n",
    "# Test the cost function with our initial predictions\n",
    "# With zero weights, all predictions are 0\n",
    "initial_cost = compute_cost(y_pred_initial, y_train)\n",
    "\n",
    "print(f\"Initial cost (with zero weights): {initial_cost:.2f}\")\n",
    "print(f\"This is high because we're predicting $0 for every car!\")\n",
    "\n",
    "# Test the gradient computation\n",
    "dW_initial, db_initial = compute_gradients(X_train_scaled, y_pred_initial, y_train)\n",
    "\n",
    "print(f\"\\nInitial gradients:\")\n",
    "print(f\"    dW shape: {dW_initial.shape}\")\n",
    "print(f\"    dW values: {dW_initial.round(2)}\")\n",
    "print(f\"    db value: {db_initial:,.2f}\")\n",
    "print(f\"\\n  (db is negative because predictions are too LOW -we need to increase bias)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f841e7bf",
   "metadata": {},
   "source": [
    "# Training Loop (Gradient Descent)\n",
    "\n",
    "Now we put it all together and train the model.\n",
    "\n",
    "Gradient descent algorithm:\n",
    "1. Make predictions with current weights\n",
    "2. Compute the cost (how wrong we are)\n",
    "3. Compute gradients (direction to improve)\n",
    "4. Update weights and bias by subtracting gradients scaled by learning rate\n",
    "5. Repeat for n_iterations\n",
    "\n",
    "We'll track the cost at each iteration to visualize the learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c26c0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Train linear regression model using gradient descent.\n",
      "\n",
      "    Parameters:\n",
      "    -----------\n",
      "    X : numpy.ndarray\n",
      "        Feature matrix of shape (n_samples, n_features)\n",
      "    y : numpy.ndarray\n",
      "        Target values of shape (n_samples,)\n",
      "    weights : numpy.ndarray\n",
      "        Initial weight vector of shape (n_features,)\n",
      "    bias : float\n",
      "        Initial bias term\n",
      "    learning_rate : float\n",
      "        Step size for gradient descent\n",
      "    n_iterations : int\n",
      "        Number of training iterations\n",
      "\n",
      "    Returns:\n",
      "    --------\n",
      "    weights : numpy.ndarray\n",
      "        Learned weight vector\n",
      "    bias : float\n",
      "        Learned bias term\n",
      "    cost_history : list\n",
      "        Cost value at each iteration (for plotting)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "def train(X, y, weights, bias, learning_rate, n_iterations):\n",
    "    \"\"\"\n",
    "    Train linear regression model using gradient descent.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Feature matrix of shape (n_samples, n_features)\n",
    "    y : numpy.ndarray\n",
    "        Target values of shape (n_samples,)\n",
    "    weights : numpy.ndarray\n",
    "        Initial weight vector of shape (n_features,)\n",
    "    bias : float\n",
    "        Initial bias term\n",
    "    learning_rate : float\n",
    "        Step size for gradient descent\n",
    "    n_iterations : int\n",
    "        Number of training iterations\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    weights : numpy.ndarray\n",
    "        Learned weight vector\n",
    "    bias : float\n",
    "        Learned bias term\n",
    "    cost_history : list\n",
    "        Cost value at each iteration (for plotting)\n",
    "    \"\"\"\n",
    "    # List to store cost at each iteration for visualization\n",
    "    cost_history = []\n",
    "\n",
    "    # Get number of samples for progress reporting\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    # Main training loop\n",
    "    for i in range(n_iterations):\n",
    "        # Step 1: Forward pass (make predictions)\n",
    "        y_pred = forward(X, weights, bias)\n",
    "\n",
    "        # Step 2: Compute cost (measure error)\n",
    "        cost = compute_cost(y_pred, y)\n",
    "        cost_history.append(cost)\n",
    "\n",
    "        # Step 3: Compute gradients\n",
    "        dW, db = compute_gradients(X, y_pred, y)\n",
    "\n",
    "        # Step 4: Update parameters\n",
    "        # Subtract gradient scaled by learning rate\n",
    "        # This moves weights in the direction that reduces cost\n",
    "        weights = weights - learning_rate * dW\n",
    "        bias = bias - learning_rate * db\n",
    "\n",
    "        # Print progress every 100 iterations\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i:4d} | Cost: {cost:,.2f}\")\n",
    "\n",
    "    # Print final iteration\n",
    "    print(f\"Iteration {n_iterations:4d} | Cost: {cost_history[-1]:,.2f}\")\n",
    "\n",
    "    return weights, bias, cost_history\n",
    "\n",
    "print(train.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72731a3b",
   "metadata": {},
   "source": [
    "# Run Training\n",
    "\n",
    "Re-initialize weights to zeros before training\n",
    "(in case we run this cell multiple times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dadcb892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "Samples: 80000\n",
      "Features: [ 0.29110069  1.72635802 -1.26971679 -0.51781295 -0.06685502 -1.14069332\n",
      " -0.22207373 -0.5072628   0.48802545  1.1780698   1.41430581]\n",
      "Learning rate: 0.01\n",
      "Iterations: 1000\n",
      "---------------------------------------------\n",
      "Iteration    0 | Cost: 284,793,019.84\n",
      "Iteration  100 | Cost: 78,691,454.99\n",
      "Iteration  200 | Cost: 54,653,021.18\n",
      "Iteration  300 | Cost: 51,457,481.33\n",
      "Iteration  400 | Cost: 51,018,396.01\n",
      "Iteration  500 | Cost: 50,955,877.80\n",
      "Iteration  600 | Cost: 50,946,407.64\n",
      "Iteration  700 | Cost: 50,944,813.11\n",
      "Iteration  800 | Cost: 50,944,499.57\n",
      "Iteration  900 | Cost: 50,944,425.76\n",
      "Iteration 1000 | Cost: 50,944,405.46\n",
      "---------------------------------------------\n",
      "Training Complete!\n",
      "\n",
      "Learned bias: $19,194.51\n",
      "\n",
      "Learned weights:\n",
      "    year           :    5191.17\n",
      "    manufacturer   :    -588.17\n",
      "    condition      :     702.61\n",
      "    cylinders      :    2401.11\n",
      "    fuel           :   -2722.32\n",
      "    odometer       :   -4482.37\n",
      "    title_status   :    -772.04\n",
      "    transmission   :    1271.16\n",
      "    drive          :   -2166.26\n",
      "    type           :      68.79\n",
      "    state          :     -21.70\n"
     ]
    }
   ],
   "source": [
    "weights = np.zeros(n_features)\n",
    "bias = 0.0\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "print(f\"Samples: {X_train_scaled.shape[0]}\")\n",
    "print(f\"Features: {X_train_scaled[1]}\")\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(f\"Iterations: {n_iterations}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Traiun the model\n",
    "weights, bias, cost_history = train(\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    weights,\n",
    "    bias,\n",
    "    learning_rate,\n",
    "    n_iterations\n",
    ")\n",
    "\n",
    "print('-' * 45)\n",
    "print(\"Training Complete!\")\n",
    "\n",
    "# Display learned parameters\n",
    "print(f\"\\nLearned bias: ${bias:,.2f}\")\n",
    "print(f\"\\nLearned weights:\")\n",
    "for i, name in enumerate(FEATURE_NAMES):\n",
    "    print(f\"    {name:<15}: {weights[i]:>10.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fdd51e",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "Now we evaluate how well our trained model performs on UNSEEN data (test data).\n",
    "Using the test set is critical - it tells us how well the model generalizes.\n",
    "\n",
    "We'll compute three metrics:\n",
    "1. MSE (Mean squared error): Average of squared errors\n",
    "2. RMSE (Root MSE): Square root of MSE - interpretable in dollars\n",
    "3. R² (Coefficient of Determination): How much variance we explain (0 to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f47fdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_sqaured_error(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Calculate Mean Squared Error.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_pred : numpy.ndarray\n",
    "        Predicted values\n",
    "    y_true : numpy.ndarray\n",
    "        Actual values\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    mse : float\n",
    "        Mean squared error\n",
    "    \"\"\"\n",
    "    # Calculate squared differences and take the mean\n",
    "    mse = np.mean((y_pred - y_true) ** 2)\n",
    "    return mse\n",
    "\n",
    "def root_mean_squared_error(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Calculate Root Mean Squared Error.\n",
    "\n",
    "    RMSE is in the same units as the target (dollars), making it interpretable.\n",
    "    An RMSE of $5000 means predictions are off by ~$5000 on average.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_pred : numpy.ndarray\n",
    "        Predicted values\n",
    "    y_true : numpy.ndarray\n",
    "        Actual values\n",
    "\n",
    "    Return:\n",
    "    -------\n",
    "    rmse : float\n",
    "        Root mean squared error\n",
    "    \"\"\"\n",
    "    # Take the square root of MSE to get back to original units\n",
    "    mse = mean_squared_error(y_pred, y_true)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "def r_squared(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Calculate R² (Coefficient of Determination).\n",
    "\n",
    "    R² measures how much of the variance in y is explained by the model.\n",
    "    - R² = 1.0: Perfect predictions\n",
    "    - R² = 0.0: Model is as good as just predicting the mean\n",
    "    - R² < 0.0: Model is worse than predicting the mean\n",
    "    \n",
    "    Formula: R² = 1 - (SS_res / SS_tot)\n",
    "    - SS_res: Sum of squared residuals (prediction errors)\n",
    "    - SS_tot: Total sum of squares (variance from mean)\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_pred : numpy.ndarray\n",
    "        Predicted values\n",
    "    y_true : numpy.ndarray\n",
    "        Actual values\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    r2 : float\n",
    "        R-squared score\n",
    "    \"\"\"\n",
    "    # SS_res: Sum of squared residuals (how far predictions are from actual)\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "\n",
    "    # SS_tot: Total sum of squares (how far actual values are from their mean)\n",
    "    # This represents the total variance in the data\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "\n",
    "    # r2 = 1 (unexplained variance / total variance)\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e094902",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML Portfolio (.venv)",
   "language": "python",
   "name": "ml-portfolio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
