{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fb4fbdf",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN) - No-Framework Implementation\n",
    "\n",
    "Multi-class classification on the **Covertype (Forest Cover Type)** dataset using pure NumPy.\n",
    "\n",
    "**Dataset**: 581,012 samples, 54 features, 7 forest cover types  \n",
    "**Task**: Predict forest cover type from cartographic variables  \n",
    "**Key Concept**: KNN is a \"lazy learner\" - no training phase, expensive at prediction time\n",
    "\n",
    "## What We'll Build From Scratch\n",
    "- **Euclidean/Manhattan distance** calculation (vectorized)\n",
    "- **K-nearest neighbor search** using argsort\n",
    "- **Majority voting** for multi-class prediction\n",
    "- **Batched prediction** to handle 464K training samples without OOM\n",
    "- **Distance-weighted voting** (optional advanced feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eea6e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete!\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Add utils to path\n",
    "sys.path.append('../..')\n",
    "from utils.data_loader import load_processed_data\n",
    "from utils.metrics import accuracy, macro_f1_score, confusion_matrix_multiclass\n",
    "from utils.visualization import (\n",
    "    plot_confusion_matrix_multiclass,\n",
    "    plot_validation_curve,\n",
    "    plot_per_class_f1\n",
    ")\n",
    "from utils.performance import track_performance\n",
    "\n",
    "print(\"Imports complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1698d25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 464,809 samples, 54 features\n",
      "Test set: 116,203 samples\n",
      "Classes (7): ['Spruce/Fir', 'Lodgepole Pine', 'Ponderosa Pine', 'Cottonwood/Willow', 'Aspen', 'Douglas-fir', 'Krummholz']\n"
     ]
    }
   ],
   "source": [
    "# Load Preprocessed data\n",
    "\"\"\"\n",
    "Data was preprocessed in data-preperation/preprocess_knn.py\n",
    "    - 80/20 split\n",
    "    - StandardScaler applied (fit on train only)\n",
    "    - All 4 frameworks load identical data for fair comparison\n",
    "\"\"\"\n",
    "\n",
    "X_train, X_test, y_train, y_test, metadata = load_processed_data('knn')\n",
    "\n",
    "# Extract metadata for reference\n",
    "class_names = metadata['class_names']\n",
    "n_classes = metadata['n_classes']\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples, {X_train.shape[1]} features\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
    "print(f\"Classes ({n_classes}): {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da68f0b",
   "metadata": {},
   "source": [
    "## Manhattan Distance Function\n",
    "\n",
    "- Scikit-Learn found that **Manhattan distance** (L1) outperforms Euclidean (L2) for this dataset.\n",
    "- We'll implement Manhattan distance from scratch.\n",
    "\n",
    "**Manhattan Distance Formula.**\n",
    "$$d(x, y) = \\sum_{i=1}^{n} |x_i - y_i|$$\n",
    "\n",
    "Sum of absolute differences across all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10b9a60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance matrix shape: (3, 5)\n",
      "Sample distances from test[0] to first 5 train samples:\n",
      "[28.32663799 16.68044501 35.29420896 31.71431377 16.84263946]\n"
     ]
    }
   ],
   "source": [
    "# Manhattan Distance (Vectorized)\n",
    "\n",
    "def manhattan_distance_batch(X_batch, X_train):\n",
    "    \"\"\"\n",
    "    Compute Manhattan (L1) distance from each sample in X_batch\n",
    "    to all samples in X_train.\n",
    "\n",
    "    Vectorized using broadcasting for efficiency:\n",
    "        - X_batch: (batch_size, n_features)\n",
    "        - X_train: (n_train, n_features)\n",
    "        - Returns: (batch_size, n_train) distance matrix\n",
    "\n",
    "    Broadcasting expands X_batch to (batch_size, 1, n_features)\n",
    "    and X_train to (1, n_train, n_features), then subtracts.\n",
    "    \"\"\"\n",
    "    diff = X_batch[:, np.newaxis, :] - X_train[np.newaxis, :, :]\n",
    "    distances = np.sum(np.abs(diff), axis=2)\n",
    "\n",
    "    return distances\n",
    "\n",
    "# Quick test with small samples\n",
    "test_distances = manhattan_distance_batch(X_test[:3], X_train[:5])\n",
    "print(f\"Distance matrix shape: {test_distances.shape}\")\n",
    "print(f\"Sample distances from test[0] to first 5 train samples:\")\n",
    "print(test_distances[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc672ba7",
   "metadata": {},
   "source": [
    "## KNN Prediction Function\n",
    "\n",
    "For each test sample, we:\n",
    "1. Compute distance to ALL training samples\n",
    "2. Find the K nearest neighbors (smallest distances)\n",
    "3. Use distance-weighted voting: closer neighbors have more influence\n",
    "\n",
    "**Distance Weighting:**\n",
    "- Weight = 1 / distance (closer = higher weight)\n",
    "- Each neighbor's vote is multiplied by its weight\n",
    "- Class with highest total weighted vote wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "080c08b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions: [2 2 2 1 2]\n",
      "Actual labels:    [2 2 1 1 2]\n"
     ]
    }
   ],
   "source": [
    "# KNN Predict with distance-weighted voting\n",
    "\n",
    "def knn_predict_batch(X_batch, X_train, y_train, k=3):\n",
    "    \"\"\"\n",
    "    Predict classes for a batch of test samples using KNN.\n",
    "\n",
    "    Uses distance-weighted voting (matches Scikit-Learn's weights='distance').\n",
    "    Closer neighbors have more influence on the prediction.\n",
    "\n",
    "    Args:\n",
    "        X_batch: Test samples (batch_size, n_features)\n",
    "        X_train: Training samples (n_train, n_features)\n",
    "        y_train: Training labels (n_labels,)\n",
    "        k: Number of neighbors to consider\n",
    "\n",
    "    Returns:\n",
    "        predictions: Predicted class for each test sample\n",
    "    \"\"\"\n",
    "    # Step 1: Compute distances from batch to all training samples\n",
    "    distances = manhattan_distance_batch(X_batch, X_train)\n",
    "\n",
    "    # Step 2: Find indices of K nearest neighbors\n",
    "    k_nearest_idx = np.argpartition(distances, k, axis=1)[:, :k]\n",
    "\n",
    "    # Get the actual distances for weighting\n",
    "    batch_indices = np.arange(len(X_batch))[:, np.newaxis]\n",
    "    k_distances = distances[batch_indices, k_nearest_idx]\n",
    "\n",
    "    # Step 3: Get labels of K nearest neighbors\n",
    "    k_labels = y_train[k_nearest_idx]\n",
    "\n",
    "    # step 4: Distance-weighted voting\n",
    "    weights = 1.0 / (k_distances + 1e-10)\n",
    "\n",
    "    # For each sample, sum weights for each class\n",
    "    predictions = []\n",
    "    unique_classes = np.unique(y_train)\n",
    "\n",
    "    for i in range(len(X_batch)):\n",
    "        class_weights = np.zeros(len(unique_classes))\n",
    "        for j, cls in enumerate(unique_classes):\n",
    "            # Sum weights where neighbor label == this class\n",
    "            mask = k_labels[i] == cls\n",
    "            class_weights[j] = np.sum(weights[i][mask])\n",
    "\n",
    "        # Predict class with highest weighted vote\n",
    "        predictions.append(unique_classes[np.argmax(class_weights)])\n",
    "\n",
    "    return np.array(predictions)\n",
    "\n",
    "# Quick test\n",
    "test_pred = knn_predict_batch(X_test[:5], X_train[:1000], y_train[:1000], k=3)\n",
    "print(f\"Test predictions: {test_pred}\")\n",
    "print(f\"Actual labels:    {y_test[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706bef29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML Portfolio (.venv)",
   "language": "python",
   "name": "ml-portfolio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
