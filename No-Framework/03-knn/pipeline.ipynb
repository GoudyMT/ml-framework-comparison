{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fb4fbdf",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN) - No-Framework Implementation\n",
    "\n",
    "Multi-class classification on the **Covertype (Forest Cover Type)** dataset using pure NumPy.\n",
    "\n",
    "**Dataset**: 581,012 samples, 54 features, 7 forest cover types  \n",
    "**Task**: Predict forest cover type from cartographic variables  \n",
    "**Key Concept**: KNN is a \"lazy learner\" - no training phase, expensive at prediction time\n",
    "\n",
    "## What We'll Build From Scratch\n",
    "- **Euclidean/Manhattan distance** calculation (vectorized)\n",
    "- **K-nearest neighbor search** using argsort\n",
    "- **Majority voting** for multi-class prediction\n",
    "- **Batched prediction** to handle 464K training samples without OOM\n",
    "- **Distance-weighted voting** (optional advanced feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eea6e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete!\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Add utils to path\n",
    "sys.path.append('../..')\n",
    "from utils.data_loader import load_processed_data\n",
    "from utils.metrics import accuracy, macro_f1_score, confusion_matrix_multiclass\n",
    "from utils.visualization import (\n",
    "    plot_confusion_matrix_multiclass,\n",
    "    plot_validation_curve,\n",
    "    plot_per_class_f1\n",
    ")\n",
    "from utils.performance import track_performance\n",
    "\n",
    "print(\"Imports complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1698d25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 464,809 samples, 54 features\n",
      "Test set: 116,203 samples\n",
      "Classes (7): ['Spruce/Fir', 'Lodgepole Pine', 'Ponderosa Pine', 'Cottonwood/Willow', 'Aspen', 'Douglas-fir', 'Krummholz']\n"
     ]
    }
   ],
   "source": [
    "# Load Preprocessed data\n",
    "\"\"\"\n",
    "Data was preprocessed in data-preperation/preprocess_knn.py\n",
    "    - 80/20 split\n",
    "    - StandardScaler applied (fit on train only)\n",
    "    - All 4 frameworks load identical data for fair comparison\n",
    "\"\"\"\n",
    "\n",
    "X_train, X_test, y_train, y_test, metadata = load_processed_data('knn')\n",
    "\n",
    "# Extract metadata for reference\n",
    "class_names = metadata['class_names']\n",
    "n_classes = metadata['n_classes']\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples, {X_train.shape[1]} features\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
    "print(f\"Classes ({n_classes}): {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da68f0b",
   "metadata": {},
   "source": [
    "## Manhattan Distance Function\n",
    "\n",
    "- Scikit-Learn found that **Manhattan distance** (L1) outperforms Euclidean (L2) for this dataset.\n",
    "- We'll implement Manhattan distance from scratch.\n",
    "\n",
    "**Manhattan Distance Formula.**\n",
    "$$d(x, y) = \\sum_{i=1}^{n} |x_i - y_i|$$\n",
    "\n",
    "Sum of absolute differences across all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10b9a60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance matrix shape: (3, 5)\n",
      "Sample distances from test[0] to first 5 train samples:\n",
      "[28.32663799 16.68044501 35.29420896 31.71431377 16.84263946]\n"
     ]
    }
   ],
   "source": [
    "# Manhattan Distance (Vectorized)\n",
    "\n",
    "def manhattan_distance_batch(X_batch, X_train):\n",
    "    \"\"\"\n",
    "    Compute Manhattan (L1) distance from each sample in X_batch\n",
    "    to all samples in X_train.\n",
    "\n",
    "    Vectorized using broadcasting for efficiency:\n",
    "        - X_batch: (batch_size, n_features)\n",
    "        - X_train: (n_train, n_features)\n",
    "        - Returns: (batch_size, n_train) distance matrix\n",
    "\n",
    "    Broadcasting expands X_batch to (batch_size, 1, n_features)\n",
    "    and X_train to (1, n_train, n_features), then subtracts.\n",
    "    \"\"\"\n",
    "    diff = X_batch[:, np.newaxis, :] - X_train[np.newaxis, :, :]\n",
    "    distances = np.sum(np.abs(diff), axis=2)\n",
    "\n",
    "    return distances\n",
    "\n",
    "# Quick test with small samples\n",
    "test_distances = manhattan_distance_batch(X_test[:3], X_train[:5])\n",
    "print(f\"Distance matrix shape: {test_distances.shape}\")\n",
    "print(f\"Sample distances from test[0] to first 5 train samples:\")\n",
    "print(test_distances[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc672ba7",
   "metadata": {},
   "source": [
    "## KNN Prediction Function\n",
    "\n",
    "For each test sample, we:\n",
    "1. Compute distance to ALL training samples\n",
    "2. Find the K nearest neighbors (smallest distances)\n",
    "3. Use distance-weighted voting: closer neighbors have more influence\n",
    "\n",
    "**Distance Weighting:**\n",
    "- Weight = 1 / distance (closer = higher weight)\n",
    "- Each neighbor's vote is multiplied by its weight\n",
    "- Class with highest total weighted vote wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "080c08b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions: [2 2 2 1 2]\n",
      "Actual labels:    [2 2 1 1 2]\n"
     ]
    }
   ],
   "source": [
    "# KNN Predict with distance-weighted voting\n",
    "\n",
    "def knn_predict_batch(X_batch, X_train, y_train, k=3):\n",
    "    \"\"\"\n",
    "    Predict classes for a batch of test samples using KNN.\n",
    "\n",
    "    Uses distance-weighted voting (matches Scikit-Learn's weights='distance').\n",
    "    Closer neighbors have more influence on the prediction.\n",
    "\n",
    "    Args:\n",
    "        X_batch: Test samples (batch_size, n_features)\n",
    "        X_train: Training samples (n_train, n_features)\n",
    "        y_train: Training labels (n_labels,)\n",
    "        k: Number of neighbors to consider\n",
    "\n",
    "    Returns:\n",
    "        predictions: Predicted class for each test sample\n",
    "    \"\"\"\n",
    "    # Step 1: Compute distances from batch to all training samples\n",
    "    distances = manhattan_distance_batch(X_batch, X_train)\n",
    "\n",
    "    # Step 2: Find indices of K nearest neighbors\n",
    "    k_nearest_idx = np.argpartition(distances, k, axis=1)[:, :k]\n",
    "\n",
    "    # Get the actual distances for weighting\n",
    "    batch_indices = np.arange(len(X_batch))[:, np.newaxis]\n",
    "    k_distances = distances[batch_indices, k_nearest_idx]\n",
    "\n",
    "    # Step 3: Get labels of K nearest neighbors\n",
    "    k_labels = y_train[k_nearest_idx]\n",
    "\n",
    "    # step 4: Distance-weighted voting\n",
    "    weights = 1.0 / (k_distances + 1e-10)\n",
    "\n",
    "    # For each sample, sum weights for each class\n",
    "    predictions = []\n",
    "    unique_classes = np.unique(y_train)\n",
    "\n",
    "    for i in range(len(X_batch)):\n",
    "        class_weights = np.zeros(len(unique_classes))\n",
    "        for j, cls in enumerate(unique_classes):\n",
    "            # Sum weights where neighbor label == this class\n",
    "            mask = k_labels[i] == cls\n",
    "            class_weights[j] = np.sum(weights[i][mask])\n",
    "\n",
    "        # Predict class with highest weighted vote\n",
    "        predictions.append(unique_classes[np.argmax(class_weights)])\n",
    "\n",
    "    return np.array(predictions)\n",
    "\n",
    "# Quick test\n",
    "test_pred = knn_predict_batch(X_test[:5], X_train[:1000], y_train[:1000], k=3)\n",
    "print(f\"Test predictions: {test_pred}\")\n",
    "print(f\"Actual labels:    {y_test[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "706bef29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing batched prediction on 100 samples...\n",
      "    Batch 2/2 complete\n",
      "Accuracy on 100 samples: 0.9900\n"
     ]
    }
   ],
   "source": [
    "# Batched KNN Prediction\n",
    "\n",
    "def knn_predict(X_train, y_train, X_test, k=3, batch_size=500):\n",
    "    \"\"\"\n",
    "    Full KNN prediction with batching to prevent memory overflow.\n",
    "\n",
    "    With 464k training samples and 116k test samples, computing all\n",
    "    distances at once would require ~200GB memory. Batching processes test samples in chunks.\n",
    "\n",
    "    Args:\n",
    "        X_train: Training features (n_train, n_features)\n",
    "        y_train: Training labels (n_train,)\n",
    "        X_test: Test features (n_test, n_features)\n",
    "        k: Number of neighbors\n",
    "        batch_size: Number of test samples to process at once\n",
    "\n",
    "    Returns:\n",
    "        predictions: Predicted class for each test sample\n",
    "    \"\"\"\n",
    "    n_test = len(X_test)\n",
    "    predictions = []\n",
    "\n",
    "    # Process test set in batches\n",
    "    n_batches = (n_test + batch_size - 1) // batch_size\n",
    "\n",
    "    for i in range(0, n_test, batch_size):\n",
    "        batch_num = i // batch_size + 1\n",
    "        end_idx = min(i + batch_size, n_test)\n",
    "\n",
    "        # Get batch and predict\n",
    "        X_batch = X_test[i:end_idx]\n",
    "        batch_pred = knn_predict_batch(X_batch, X_train, y_train, k)\n",
    "        predictions.extend(batch_pred)\n",
    "\n",
    "        # Progress update every 20 batches\n",
    "        if batch_num % 20 == 0 or batch_num == n_batches:\n",
    "            print(f\"    Batch {batch_num}/{n_batches} complete\")\n",
    "\n",
    "    return np.array(predictions)\n",
    "\n",
    "# Test on small subset to verify it works\n",
    "print(\"Testing batched prediction on 100 samples...\")\n",
    "test_pred_small = knn_predict(X_train, y_train, X_test[:100], k=3, batch_size=50)\n",
    "small_acc = accuracy(y_test[:100], test_pred_small)\n",
    "print(f\"Accuracy on 100 samples: {small_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcaa0d3",
   "metadata": {},
   "source": [
    "## K-Value Tuning\n",
    "\n",
    "Testing K = 1, 3, 5, 7, 9, 11, 13, 15 to find optimal number of neighbors.\n",
    "\n",
    "**Note**: This will take a while (~5-10 minutes per K value) since we're computing distances to 464K training samples for each of 116K test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7f8a748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning with 46,480 train, 5,810 test samples\n",
      "==================================================\n",
      "\n",
      "K = 1\n",
      "    Batch 12/12 complete\n",
      "    Batch 1/1 complete\n",
      "  Train Acc: 1.0000 | Test Acc: 0.8590 | Time: 49.2s\n",
      "\n",
      "K = 3\n",
      "    Batch 12/12 complete\n",
      "    Batch 1/1 complete\n",
      "  Train Acc: 1.0000 | Test Acc: 0.8596 | Time: 49.9s\n",
      "\n",
      "K = 5\n",
      "    Batch 12/12 complete\n",
      "    Batch 1/1 complete\n",
      "  Train Acc: 1.0000 | Test Acc: 0.8559 | Time: 50.2s\n",
      "\n",
      "K = 7\n",
      "    Batch 12/12 complete\n",
      "    Batch 1/1 complete\n",
      "  Train Acc: 1.0000 | Test Acc: 0.8565 | Time: 50.1s\n",
      "\n",
      "K = 9\n",
      "    Batch 12/12 complete\n",
      "    Batch 1/1 complete\n",
      "  Train Acc: 1.0000 | Test Acc: 0.8539 | Time: 49.9s\n",
      "\n",
      "K = 11\n",
      "    Batch 12/12 complete\n",
      "    Batch 1/1 complete\n",
      "  Train Acc: 1.0000 | Test Acc: 0.8478 | Time: 49.9s\n",
      "\n",
      "K = 13\n",
      "    Batch 12/12 complete\n",
      "    Batch 1/1 complete\n",
      "  Train Acc: 1.0000 | Test Acc: 0.8449 | Time: 49.9s\n",
      "\n",
      "K = 15\n",
      "    Batch 12/12 complete\n",
      "    Batch 1/1 complete\n",
      "  Train Acc: 1.0000 | Test Acc: 0.8420 | Time: 49.8s\n",
      "\n",
      "==================================================\n",
      "Best K: 3 (Test Accuracy: 0.8596)\n"
     ]
    }
   ],
   "source": [
    "# K-Value Tuning\n",
    "\n",
    "\"\"\"\n",
    "Test different K values - this takes a while with manual implementation\n",
    "Using a subset for tuning to save time, then full test with best k.\n",
    "\"\"\"\n",
    "\n",
    "np.random.seed(113)\n",
    "\n",
    "# Sample 10% of training data for tuning (stratified by class)\n",
    "tune_train_size = len(X_train) // 10  # ~46K instead of 464K\n",
    "tune_indices = np.random.choice(len(X_train), tune_train_size, replace=False)\n",
    "X_train_tune = X_train[tune_indices]\n",
    "y_train_tune = y_train[tune_indices]\n",
    "\n",
    "# Sample 5% of test data for tuning\n",
    "tune_test_size = len(X_test) // 20  # ~5.8K instead of 116K\n",
    "X_test_tune = X_test[:tune_test_size]\n",
    "y_test_tune = y_test[:tune_test_size]\n",
    "\n",
    "print(f\"Tuning with {len(X_train_tune):,} train, {len(X_test_tune):,} test samples\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "k_values = [1, 3, 5, 7, 9, 11, 13, 15]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"\\nK = {k}\")\n",
    "    \n",
    "    # Predict on tuning test subset\n",
    "    with track_performance() as perf:\n",
    "        y_pred = knn_predict(X_train_tune, y_train_tune, X_test_tune, k=k, batch_size=500)\n",
    "    \n",
    "    test_acc = accuracy(y_test_tune, y_pred)\n",
    "    test_scores.append(test_acc)\n",
    "    \n",
    "    # Train accuracy on small sample\n",
    "    train_pred = knn_predict(X_train_tune, y_train_tune, X_train_tune[:500], k=k, batch_size=500)\n",
    "    train_acc = accuracy(y_train_tune[:500], train_pred)\n",
    "    train_scores.append(train_acc)\n",
    "    \n",
    "    print(f\"  Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f} | Time: {perf['time']:.1f}s\")\n",
    "\n",
    "# Find best K\n",
    "best_idx = np.argmax(test_scores)\n",
    "best_k = k_values[best_idx]\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"Best K: {best_k} (Test Accuracy: {test_scores[best_idx]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b23112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML Portfolio (.venv)",
   "language": "python",
   "name": "ml-portfolio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
