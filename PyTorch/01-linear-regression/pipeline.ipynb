{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b2d89be",
   "metadata": {},
   "source": [
    "# PyTorch Linear Regression\n",
    "\n",
    "### This implementation uses PyTorch to build a linear regression model\n",
    "\n",
    "### Goal: Predict used car prices and compare with No-Framework and Scikit-Learn\n",
    "\n",
    "What PyTorch provides (That we built manually in No-Framework)\n",
    "- `torch.Tensor`: GPU-compatible arrays that track gradients automatically\n",
    "- `torch.nn.Linear`: Encapsulates weights and bias in a single layer\n",
    "- `torch.nn.MSELoss`: Pre-built loss function (replaces our manual compute_cost)\n",
    "- `torch.optim.SGD`: Optimizer that handles parameter updates (replaces manual gradient descent)\n",
    "- `auotgrad`: Automatic differentiation - computes gradients via .backward()\n",
    "\n",
    "Key Concept - Autograd:\n",
    "- In No-Framework, we manually computed gradients\n",
    "- In PyTorch, we jull call loss.backward() and gradients are computed automatically\n",
    "- This is the foundation of modern deep learning - same math, zero manual calculus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d30b4fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n",
      "PyTorch version: 2.10.0+cpu\n",
      "Random seed set to : 113\n"
     ]
    }
   ],
   "source": [
    "# torch: The main PyTorch library for tensor operations and neural networks\n",
    "import torch\n",
    "\n",
    "# torch.nn: Neural network module containing layers, loss function, etc.\n",
    "# We import it as 'nn' for shorter syntax\n",
    "import torch.nn as nn\n",
    "\n",
    "# torch.optim: Optimization algorithms (SGD, Adam, etc.)\n",
    "# These handle the weights updates we did manually in No-Framework\n",
    "import torch.optim as optim\n",
    "\n",
    "# numpy: Still needed for initial data handling before converting to tensors\n",
    "import numpy as np\n",
    "\n",
    "#pandas: For loading CSV data\n",
    "import pandas as pd\n",
    "\n",
    "# matplotlib: for visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# os: File path handling\n",
    "import os\n",
    "# Sklearn utilities: Using these for consistency with  Scikit-Learn implementation\n",
    "# This ensures identical train/test splits and scaling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Performance tracking\n",
    "import time\n",
    "import tracemalloc\n",
    "import platform\n",
    "\n",
    "# Set random seeds for reporducibility\n",
    "# We set seeds for BOTH numpy and torch to ensure consistent results\n",
    "RANDOM_SEED = 113\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Random seed set to : {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b112aa4",
   "metadata": {},
   "source": [
    "# Load Cleaned Data\n",
    "\n",
    "- Load the same pre-processed dataset used in NF and SL\n",
    "- Using pandas for consistency with SL implementation\n",
    "- This ensures fair comparison across all frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13e9b965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (100000, 12)\n",
      "Columns: ['price', 'year', 'manufacturer', 'condition', 'cylinders', 'fuel', 'odometer', 'title_status', 'transmission', 'drive', 'type', 'state']\n",
      "\n",
      "First 3 rows:\n",
      "   price    year  manufacturer  condition  cylinders  fuel  odometer  \\\n",
      "0  29990  2014.0             7          2          6     2   26129.0   \n",
      "1   6995  2006.0            12          0          6     2  198947.0   \n",
      "2   4995  2009.0            35          6          8     2  152794.0   \n",
      "\n",
      "   title_status  transmission  drive  type  state  \n",
      "0             0             2      0     8     17  \n",
      "1             6             0      3    10      5  \n",
      "2             0             0      3    11     22  \n"
     ]
    }
   ],
   "source": [
    "# Define path to our cleaned dataset\n",
    "DATA_PATH = os.path.join('..', '..', 'data', 'processed', 'vehicles_clean.csv')\n",
    "\n",
    "# Load data using pandas\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Verify data loaded correctly\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68582b8",
   "metadata": {},
   "source": [
    "# Separate Features and Target\n",
    "\n",
    "- Price is our TARGET variable \n",
    "- All other columns are FEATURES\n",
    "- Same seperation as Scikit-Learn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43afc6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features (X) shape: (100000, 11)\n",
      "Target (y) shape: (100000,)\n",
      "\n",
      "Feature Names: ['year', 'manufacturer', 'condition', 'cylinders', 'fuel', 'odometer', 'title_status', 'transmission', 'drive', 'type', 'state']\n"
     ]
    }
   ],
   "source": [
    "# Define target and column variable (same as scikit-learn)\n",
    "TARGET_COLUMN = 'price'\n",
    "FEATURE_COLUMNS = [ 'year', 'manufacturer', 'condition', 'cylinders',\n",
    "                    'fuel', 'odometer', 'title_status', 'transmission', 'drive', 'type', 'state']\n",
    "\n",
    "# Extract target (y) and features (X) as numpy arrays\n",
    "y = df[TARGET_COLUMN].values\n",
    "X = df[FEATURE_COLUMNS].values\n",
    "\n",
    "# Store feature names for later use (displaying learned weights)\n",
    "FEATURE_NAMES = FEATURE_COLUMNS\n",
    "\n",
    "# Verify shapes \n",
    "print(f\"Features (X) shape: {X.shape}\")\n",
    "print(f\"Target (y) shape: {y.shape}\")\n",
    "print(f\"\\nFeature Names: {FEATURE_NAMES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de051596",
   "metadata": {},
   "source": [
    "# Train/Test Split\n",
    "\n",
    "- Using Sklearn's train_test_split for consistency with Scikit-learn implementation\n",
    "- Same 80/20 split, same random seed (113)\n",
    "- This ensures we're comparing apples-to-applies across frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "403d9dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 80,000 samples (80%)\n",
      "Test set size: 20,000 samples (20%)\n",
      "\n",
      "X_train shape: (80000, 11)\n",
      "X_test shape: (20000, 11)\n",
      "y_train shape: (80000,)\n",
      "y_test shape: (20000,)\n"
     ]
    }
   ],
   "source": [
    "# Split data using sklearn (same as Scikit-Learn)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,                          # Features to split\n",
    "    y,                          # Target to split\n",
    "    test_size=0.2,              # 20% for testing, 80% for training\n",
    "    random_state=RANDOM_SEED    # Seed 113 for reporoducibility\n",
    ")\n",
    "\n",
    "# Verify split sizes\n",
    "print(f\"Training set size: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.0f}%)\")\n",
    "print(f\"Test set size: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.0f}%)\")\n",
    "print(f\"\\nX_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43384a7e",
   "metadata": {},
   "source": [
    "# Feature Scaling (Z-Score Normalization)\n",
    "\n",
    "- Using sklearn's StandardScaler for consistency with Scikit-Learn implementation\n",
    "- Same z-score noramlization\n",
    "- Fit on training data only, transform both train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0512c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling parameters (computed from training data):\n",
      "\n",
      "Feature                    Mean             Std\n",
      "-----------------------------------------------\n",
      "year                    2012.32            5.79\n",
      "manufacturer              18.24           11.48\n",
      "condition                  3.09            2.43\n",
      "cylinders                  6.00            1.92\n",
      "fuel                       2.05            0.78\n",
      "odometer               94235.84        62977.76\n",
      "title_status               0.23            1.06\n",
      "transmission               0.39            0.77\n",
      "drive                      1.40            1.21\n",
      "type                       7.14            4.12\n",
      "state                     23.60           15.10\n",
      "\n",
      "--- Verification (Training Data After Scaling) ---\n",
      "Mean of each feature (should be 0): [ 0.  0.  0. -0.  0. -0. -0.  0.  0. -0. -0.]\n",
      "Std of each feature (should be 1): [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Create and fit scaler on training data (same as Scikit-Learn)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit_transform on training data: calculates mean/std AND applies scaling\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# transform on test data: uses mean/std from training (no data leakage)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Display the learned scaling parameters\n",
    "print(\"Scaling parameters (computed from training data):\\n\")\n",
    "print(f\"{'Feature':<15} {'Mean':>15} {'Std':>15}\")\n",
    "print(\"-\" * 47)\n",
    "for i, name in enumerate(FEATURE_NAMES):\n",
    "    print(f\"{name:<15} {scaler.mean_[i]:>15.2f} {scaler.scale_[i]:>15.2f}\")\n",
    "\n",
    "# Verify scaling worked - training data should have mean=0 and std=1\n",
    "print(\"\\n--- Verification (Training Data After Scaling) ---\")\n",
    "print(f\"Mean of each feature (should be 0): {np.mean(X_train_scaled, axis=0).round(6)}\")\n",
    "print(f\"Std of each feature (should be 1): {np.std(X_train_scaled, axis=0).round(6)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24b22d5",
   "metadata": {},
   "source": [
    "# Convert to PyTorch Tensors\n",
    "\n",
    "**NEW PYTORCH CONCEPT: Tensors**\n",
    "\n",
    "Tensors are PyTorch's fundamental data structure (like NumPy arrays but with superpowers):\n",
    "- Can run on GPU for massive speedups\n",
    "- Track operations for automatic gradient computation (autograd)\n",
    "- Required format for all PyTorch models\n",
    "\n",
    "Key considerations:\n",
    "- Pytorch expects `float32` dtype (not float64 which NumPy uses by default)\n",
    "- Target (y) must be reshaped from (n,) to (n, 1) for PyTorch's loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c38dd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor shapes:\n",
      "    X_train_tensor: torch.Size([80000, 11])\n",
      "    X_test_tensor:  torch.Size([20000, 11])\n",
      "    y_train_tensor: torch.Size([80000, 1])\n",
      "    y_test_tensor:  torch.Size([20000, 1])\n",
      "\n",
      "Tensor dtypes:\n",
      "    x_train_tensor: torch.float32\n",
      "    y_train_tensor: torch.float32\n",
      "\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Convert NumPy arrays to PyTorch tensors\n",
    "\n",
    "# torch.tensor() creates a tensor from data\n",
    "# dtype=torch.float32 is REQUIRED because:\n",
    "# 1. PyTorch defaults to float32 for efficiency (uses less memory than float64)\n",
    "# 2. Neural network layers expect flaot32 inputs\n",
    "# 3. GPU operations are optimized for float32\n",
    "\n",
    "# Convert features (X) to tensors\n",
    "# Shapes stay the same (80000, 11) and (20000, 11)\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "\n",
    "# Convert target (y) to tensors\n",
    "# IMPORTANT: Reshape (n,) to (n, 1) using .reshape(-1, 1)\n",
    "# -1 means \"infer this dimension\" (PyTorch figures out it's 80000)\n",
    "# 1 means \"one column\"\n",
    "# This reshaping is needed because:\n",
    "# - nn.Linear outputs shape (n, 1)\n",
    "# - Loss function needs y_pred and y_true to have same shape\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "# Verify tensor shapes and types\n",
    "print(\"Tensor shapes:\")\n",
    "print(f\"    X_train_tensor: {X_train_tensor.shape}\")\n",
    "print(f\"    X_test_tensor:  {X_test_tensor.shape}\")\n",
    "print(f\"    y_train_tensor: {y_train_tensor.shape}\")\n",
    "print(f\"    y_test_tensor:  {y_test_tensor.shape}\")\n",
    "\n",
    "print(f\"\\nTensor dtypes:\")\n",
    "print(f\"    x_train_tensor: {X_train_tensor.dtype}\")\n",
    "print(f\"    y_train_tensor: {y_train_tensor.dtype}\")\n",
    "\n",
    "# Show that tensors are on CPU (we're not using GPU for this project)\n",
    "print(f\"\\nDevice: {X_train_tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa2fb68",
   "metadata": {},
   "source": [
    "# Define the Model\n",
    "\n",
    "PyTorch uses `nn.Module` as the base class for all neural networks. For linear regression, we use `nn.Linear` which encapsulates:\n",
    "- **Weights**: Automatically initialized (no manual `np.zeros()` needed)\n",
    "- **Bias**: Included by default\n",
    "- **Forward pass**: Built-in matrix multiplication\n",
    "\n",
    "| PyTorch | No-Framework Equivalent |\n",
    "|---------|------------------------|\n",
    "| `nn.Linear(11, 1)` | `weights = np.zeros(11)` + `bias = 0` |\n",
    "| `model(X)` | `X @ weights + bias` |\n",
    "| `model.parameters()` | Manual weight/bias variables |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3510551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture\n",
      "Linear(in_features=11, out_features=1, bias=True)\n",
      "\n",
      "Learnable Parameters:\n",
      "    weight: shape torch.Size([1, 11])\n",
      "    bias: shape torch.Size([1])\n",
      "\n",
      "Initial Weight Values (first 5):\n",
      "    tensor([-0.2362,  0.0226,  0.2529,  0.1047, -0.2397])\n",
      "\n",
      "Initial Bias Value:\n",
      "    tensor([-0.0643])\n"
     ]
    }
   ],
   "source": [
    "# DEFINE THE MODEL\n",
    "\n",
    "# nn.Linear creates a fully connected layer:\n",
    "# - First argument (11): number of input features\n",
    "# - Second argument (1): number of outputs (1 for regression)\n",
    "# - Automatically intializes weights and bias internally\n",
    "\n",
    "# Define a simple linear regression model using nn.Linear\n",
    "# This replaces our manual weights = np.zeros() and bias = 0 from No-Framework\n",
    "model = nn.Linear(in_features=11, out_features=1)\n",
    "\n",
    "# Inspect the model architecture\n",
    "print(\"Model Architecture\")\n",
    "print(model)\n",
    "print()\n",
    "\n",
    "# View the learnable parameters (weights and bias)\n",
    "# model.parameters() returns as iterator over all trainable parameters\n",
    "print(\"Learnable Parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    # name: 'weight' or 'bias'\n",
    "    # param: the actual tensor containing values\n",
    "    print(f\"    {name}: shape {param.shape}\")\n",
    "print()\n",
    "\n",
    "# Show initial weight values (randomly initialized, not zeros like No-Framework)\n",
    "print(\"Initial Weight Values (first 5):\")\n",
    "print(f\"    {model.weight.data[0, :5]}\")\n",
    "print(f\"\\nInitial Bias Value:\")\n",
    "print(f\"    {model.bias.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaeba3a",
   "metadata": {},
   "source": [
    "# Loss Function and Optimizer\n",
    "\n",
    "PyTorch separates the \"what to minimize\" (loss function) from \"how to minimize it\" (optimizer):\n",
    "\n",
    "- **Loss Function (`nn.MSELoss`)**: Calculates Mean Squared Error, same as our No-Framework cost function\n",
    "- **Optimizer (`optim.SGD`)**: Performs gradient descent, replacing our manual weight update loop\n",
    "\n",
    "| PyTorch | No-Framework Equivalent |\n",
    "|---------|------------------------|\n",
    "| `nn.MSELoss()` | `cost = (1/m) * np.sum((predictions - y)**2)` |\n",
    "| `optim.SGD(params, lr)` | `weights -= learning_rate * gradient` |\n",
    "| `optimizer.step()` | Manual weight/bias updates |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a116f161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Function: MSELoss()\n",
      "Optimizer: SGD with learning_rate= 0.01\n",
      "\n",
      "No-Framework Comparison:\n",
      "    Our manual gradient descent used the same lr=0.01\n",
      "    PyTorch automates the weight updates I coded manually\n"
     ]
    }
   ],
   "source": [
    "# LOSS FUNCTION AND OPTIMIZER\n",
    "\n",
    "# Define the loss function (what we want to minimize)\n",
    "# nn.MSELoss computes Mean Squared Error\n",
    "# This is identical to the No-Framework cost function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer (how we minimize the loss)\n",
    "# SGD = Stochastic Gradient Descent (same algorithm as No-Framework)\n",
    "# Parameters:\n",
    "# - model.parameters(): tells optimizer which values to update\n",
    "# - lr: learning rate (same 0.01 used in No-Framework)\n",
    "learning_rate = 0.01\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Display configuration\n",
    "print(\"Loss Function:\", criterion)\n",
    "print(f\"Optimizer: SGD with learning_rate= {learning_rate}\")\n",
    "print(f\"\\nNo-Framework Comparison:\")\n",
    "print(f\"    Our manual gradient descent used the same lr={learning_rate}\")\n",
    "print(f\"    PyTorch automates the weight updates I coded manually\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f0b7a1",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "\n",
    "The PyTorch training loop follows a consistent pattern for every model:\n",
    "\n",
    "1. **Forward pass**: `predictions = model(X)` - compute outputs\n",
    "2. **Compute loss**: `loss = criterion(predictions, y)` - measure error\n",
    "3. **Zero gradients**: `optimizer.zero_grad()` - clear old gradients\n",
    "4. **Backward pass**: `loss.backward()` - compute new gradients (autograd!)\n",
    "5. **Update weights**: `optimizer.step()` - apply gradient descent\n",
    "\n",
    "| PyTorch | No-Framework Equivalent |\n",
    "|---------|------------------------|\n",
    "| `model(X_train)` | `X @ weights + bias` |\n",
    "| `loss.backward()` | `compute_gradients()` (manual math) |\n",
    "| `optimizer.step()` | `weights -= lr * dW; bias -= lr * dB` |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfd7e8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Linear Regression Model...\n",
      "==================================================\n",
      "    Iteration  100/1000 | Loss: 109227488.0000\n",
      "    Iteration  200/1000 | Loss: 101801136.0000\n",
      "    Iteration  300/1000 | Loss: 101656936.0000\n",
      "    Iteration  400/1000 | Loss: 101653168.0000\n",
      "    Iteration  500/1000 | Loss: 101652968.0000\n",
      "    Iteration  600/1000 | Loss: 101652944.0000\n",
      "    Iteration  700/1000 | Loss: 101652944.0000\n",
      "    Iteration  800/1000 | Loss: 101652944.0000\n",
      "    Iteration  900/1000 | Loss: 101652952.0000\n",
      "    Iteration 1000/1000 | Loss: 101652944.0000\n",
      "==================================================\n",
      "Training Complete!\n",
      "    Initial Loss:   569607296.0000\n",
      "    Final Loss:     101652944.0000\n"
     ]
    }
   ],
   "source": [
    "# TRAINING LOOP\n",
    "\n",
    "# Hyperparameters (same as NF for fair comparison)\n",
    "num_iterations = 1000\n",
    "print_every = 100\n",
    "\n",
    "# Store loss history for plotting\n",
    "loss_history = []\n",
    "\n",
    "print(\"Training Linear Regression Model...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Training loop - iterates num_iterations time\n",
    "for iteration in range(num_iterations):\n",
    "    # Step 1: Forward pass\n",
    "    # Pass training data through the model to get predictions\n",
    "    predictions = model(X_train_tensor)\n",
    "\n",
    "    # Step 2: Compute loss\n",
    "    # Calculate MSE between predictions and actual values\n",
    "    loss = criterion(predictions, y_train_tensor)\n",
    "\n",
    "    # Step 3: Zero Gradients\n",
    "    # Clear gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Step 4: Backward Pass (AUTOGRAD)\n",
    "    # Automatically compute gradients of loss w.r.t. all parameters\n",
    "    # This replaces my entire compute_gradients() function\n",
    "    loss.backward()\n",
    "\n",
    "    # Step 5: Update weights - Apply gradient descent\n",
    "    optimizer.step()\n",
    "\n",
    "    # Store loss for plotting (convert tensor to Python float)\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "    # Print progress every print_every iterations\n",
    "    if (iteration + 1) % print_every == 0:\n",
    "        print(f\"    Iteration {iteration + 1:4d}/{num_iterations} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training Complete!\")\n",
    "print(f\"    Initial Loss:   {loss_history[0]:.4f}\")\n",
    "print(f\"    Final Loss:     {loss_history[-1]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML Portfolio (.venv)",
   "language": "python",
   "name": "ml-portfolio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
