{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "899f12c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 454,902 samples, 30 features\n",
      "Test: 56,962 samples\n",
      "Class balance - Train: 50.0% fraud\n",
      "Class balance - Test: 0.2% fraud\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Logistic Regression - PyTorch Implementation\n",
    "Using autograd for automatic gradient computation.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Self created utilities\n",
    "from utils.metrics import accuracy, precision, recall, f1_score, auc_score\n",
    "from utils.performance import track_performance\n",
    "from utils.visualization import (\n",
    "    plot_cost_curve,\n",
    "    plot_confusion_matrix,\n",
    "    plot_roc_curve,\n",
    "    plot_feature_importance\n",
    ")\n",
    "\n",
    "# Load preprocessed data (already scaled, SMOTE applied, 50/50 balanced)\n",
    "X_train_np = np.load('../../data/processed/logistic_regression/X_train.npy')\n",
    "X_test_np = np.load('../../data/processed/logistic_regression/X_test.npy')\n",
    "y_train_np = np.load('../../data/processed/logistic_regression/y_train.npy')\n",
    "y_test_np = np.load('../../data/processed/logistic_regression/y_test.npy')\n",
    "\n",
    "# Load metadata for feature names\n",
    "with open('../../data/processed/logistic_regression/preprocessing_info.json') as f:\n",
    "    meta = json.load(f)\n",
    "feature_names = meta['feature_names']\n",
    "\n",
    "# NEW SECTION\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train_np, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test_np, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train_np, dtype=torch.float32).reshape(-1, 1)\n",
    "y_test = torch.tensor(y_test_np, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "print(f\"Training: {X_train.shape[0]:,} samples, {X_train.shape[1]} features\")\n",
    "print(f\"Test: {X_test.shape[0]:,} samples\")\n",
    "print(f\"Class balance - Train: {y_train.mean().item():.1%} fraud\")\n",
    "print(f\"Class balance - Test: {y_test.mean().item():.1%} fraud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8116719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epcoh  100 | Loss: 0.149933\n",
      "Epcoh  200 | Loss: 0.101981\n",
      "Epcoh  300 | Loss: 0.084414\n",
      "Epcoh  400 | Loss: 0.075295\n",
      "Epcoh  500 | Loss: 0.069664\n",
      "Epcoh  600 | Loss: 0.065812\n",
      "Epcoh  700 | Loss: 0.062994\n",
      "Epcoh  800 | Loss: 0.060834\n",
      "Epcoh  900 | Loss: 0.059119\n",
      "Epcoh 1000 | Loss: 0.057721\n",
      "\n",
      "Training complete!\n",
      "Time: 2.36 sec | Memory: 0.05 MB\n"
     ]
    }
   ],
   "source": [
    "# Model Definition\n",
    "\n",
    "# Single layer: 30 inputs ; 1 output\n",
    "# PyTorch's nn.Linear handles weights and bias automatically\n",
    "model = nn.Linear(in_features=30, out_features=1)\n",
    "\n",
    "# BCEWithLogisticLoss combines sigmoid + BCE for numerical stability\n",
    "# It applies sigmoid internally, so we don't need a separate activation\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# SGD optimizer - same as the manual gradient descent\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1) \n",
    "\n",
    "# Training loop\n",
    "\n",
    "n_epochs = 1000\n",
    "cost_history = []\n",
    "\n",
    "with track_performance() as perf:\n",
    "    for epoch in range(n_epochs):\n",
    "        # Forward pass: compute logits (raw scores before sigmoid)\n",
    "        logits = model(X_train) \n",
    "\n",
    "        # Compute loss (sigmoid applied internally by BCEWithLogitsLoss)\n",
    "        loss = criterion(logits, y_train)\n",
    "        cost_history.append(loss.item())\n",
    "\n",
    "        # Backward pass: compute gradients automatically\n",
    "        optimizer.zero_grad()   # Clear previous gradients\n",
    "        loss.backward()         # Compute gradients via autograd\n",
    "        optimizer.step()        # Update weights\n",
    "\n",
    "        # Print progress every 100 epochs\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"Epcoh {epoch+1:4d} | Loss: {loss.item():.6f}\")\n",
    "\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Time: {perf['time']:.2f} sec | Memory: {perf['memory']:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ae90c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "TEST SET RESULTS\n",
      "========================================\n",
      "Accuracy:  0.9890\n",
      "Precision: 0.1169\n",
      "Recall:    0.8265\n",
      "F1 Score:  0.2048\n",
      "AUC-ROC:   0.9034\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on Test Set\n",
    "\n",
    "# Set model to evaluation mode (disable dropout, etc. - not used here but good practice for me)\n",
    "model.eval()\n",
    "\n",
    "# Get predictions (no gradient tracking need for inference)\n",
    "with torch.no_grad():\n",
    "    logits_test = model(X_test)\n",
    "    y_proba_tensor = torch.sigmoid(logits_test)     # Convert logits to probabilities\n",
    "\n",
    "# Convert to NumPy for our metrics functions\n",
    "y_proba = y_proba_tensor.numpy().flatten()\n",
    "y_pred = (y_proba >= 0.5).astype(int)\n",
    "y_test_np = y_test.numpy().flatten()\n",
    "\n",
    "# Calculate metrics using the shared utilities\n",
    "acc = accuracy(y_test_np, y_pred)\n",
    "prec = precision(y_test_np, y_pred)\n",
    "rec = recall(y_test_np, y_pred)\n",
    "f1 = f1_score(y_test_np, y_pred)\n",
    "auc = auc_score(y_test_np, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"=\" * 40)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Accuracy:  {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall:    {rec:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "print(f\"AUC-ROC:   {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bf9415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML Portfolio (.venv)",
   "language": "python",
   "name": "ml-portfolio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
