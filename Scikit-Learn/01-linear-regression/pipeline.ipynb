{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05e601ec",
   "metadata": {},
   "source": [
    "# Scikit-Learn Linear Regression\n",
    "\n",
    "### This implementation uses Scikit-Learn to build a linear regression model\n",
    "\n",
    "## Goal: Predict used car prices and compare with No-Framework implementation\n",
    "\n",
    "What Scikit-Learn provides (what we built manually in No-Framework):\n",
    "- `train_test_split()`: Replaces our manual `train_test_split()` function\n",
    "- `StandardScaler`: Replaces our manual `compute_scaling_params()` and `scale_features()` functions\n",
    "- `LinearRegression`: Replaces our manual `foward()`, `compute_cost()`, `compute_gradients()`, and `train()` functions\n",
    "- `mean_squared_error`, `r2_score`: Replaces our manual metric functions\n",
    "\n",
    "Key Differerence: Scikit-Learn uses the **Normal Equation** (closed-form solution) instead of gradient descent. This means:\n",
    "- No learning rate to tune\n",
    "- No iterations to set\n",
    "- Instant optimal solution (for datasets that fit in memory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75e139f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6f21d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n",
      "Random seed set to: 113\n"
     ]
    }
   ],
   "source": [
    "# pandas: Data manipulation library for loading CSV files into dataframes\n",
    "# We use pandas instead of numpy.genfromtxt() for easier column access\n",
    "import pandas as pd\n",
    "\n",
    "# numpy: Numerical operations library\n",
    "# Still needed for array operations and setting random seed\n",
    "import numpy as np\n",
    "\n",
    "# matplotlib.pyplot: Plotting library for creating visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# os: Operating system interface for file path handling\n",
    "import os\n",
    "\n",
    "# Scikit-Learn Imports\n",
    "\n",
    "# train_test_split: Splits array into random train and test subsets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# StandardScaler: Standardizes features using z-score normalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# LinearRegression: Ordinary leas squares Linear Regression\n",
    "# KEY DIFFERENCES: Uses normal Equation instead of gradient descent\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# mean_squared_error: Calculates MSE between predictions and actual values\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Set random seed for reproducabiility (matches our project-wide seed)\n",
    "RANDOM_SEED = 113\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"Random seed set to: {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88151537",
   "metadata": {},
   "source": [
    "# Load cleaned data\n",
    "\n",
    "- Load the same pre-processed dataset used in NF implementation\n",
    "- This ensures a fair comparison between frameworks\n",
    "- Using pandas instead of numpy.genfromtxt() for easier column handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb192b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (100000, 12)\n",
      "Columns: ['price', 'year', 'manufacturer', 'condition', 'cylinders', 'fuel', 'odometer', 'title_status', 'transmission', 'drive', 'type', 'state']\n",
      "\n",
      "First 3 rows:\n",
      "   price    year  manufacturer  condition  cylinders  fuel  odometer  \\\n",
      "0  29990  2014.0             7          2          6     2   26129.0   \n",
      "1   6995  2006.0            12          0          6     2  198947.0   \n",
      "2   4995  2009.0            35          6          8     2  152794.0   \n",
      "\n",
      "   title_status  transmission  drive  type  state  \n",
      "0             0             2      0     8     17  \n",
      "1             6             0      3    10      5  \n",
      "2             0             0      3    11     22  \n"
     ]
    }
   ],
   "source": [
    "# Define path to our cleaned dataset\n",
    "DATA_PATH = os.path.join('..', '..', 'data', 'processed', 'vehicles_clean.csv')\n",
    "\n",
    "# pd.read_csv() reads a csv file into a pandas dataframe\n",
    "# Unlike numpy.genfromtxt(), pandas:\n",
    "# - Automatically detects and preserves column headers\n",
    "# - Infers data types for each column\n",
    "# - Provides easy column access via df['column_name']\n",
    "# - Handles missing values gracefully\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# df.sahpe returns (rows, columns) as a tuple\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# df.column.tolist() converts column names to a python list\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# df.head(n) returns the first n rows as a DataFrame\n",
    "# Useful for quick visual verification that data loaded correctly\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fbb3e4",
   "metadata": {},
   "source": [
    "# Seperate Features and Target\n",
    "\n",
    "- price (column 0) is our TARGET - what we want to predict\n",
    "- All other columns (1-11) are FEATURES - inputs to our model\n",
    "- Using pandas column selection instead of numpy array slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d5858dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features (X) shape: (100000, 11)\n",
      "Target (y) shape: (100000,)\n",
      "\n",
      "Feature Names: ['year', 'manufacturer', 'condition', 'cylinders', 'fuel', 'odometer', 'title_status', 'transmission', 'drive', 'type', 'state']\n"
     ]
    }
   ],
   "source": [
    "# Define which column is our target variable\n",
    "TARGET_COLUMN = 'price'\n",
    "\n",
    "# Define feature column names\n",
    "FEATURE_COLUMNS = [ 'year', 'manufacturer', 'condition', 'cylinders',\n",
    "                    'fuel', 'odometer', 'title_status', 'transmission', 'drive', 'type', 'state']\n",
    "\n",
    "# Extract target variable (y) using pandas column selection\n",
    "# df[column_name] returns a pandas Series (1D array with labels)\n",
    "# .values converst it to a numpy array (matches NF format)\n",
    "y = df[TARGET_COLUMN].values\n",
    "\n",
    "# Extract features (X) using pandas column select\n",
    "X = df[FEATURE_COLUMNS].values\n",
    "\n",
    "# Verify shapes match No-Framework\n",
    "# X should be (100000, 11)\n",
    "# y should be (100000,)\n",
    "print(f\"Features (X) shape: {X.shape}\")\n",
    "print(f\"Target (y) shape: {y.shape}\")\n",
    "\n",
    "# Store feature names for later use (displaying learned weights)\n",
    "FEATURE_NAMES = FEATURE_COLUMNS\n",
    "print(f\"\\nFeature Names: {FEATURE_COLUMNS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0c913a",
   "metadata": {},
   "source": [
    "# Train/Test Split\n",
    "\n",
    "- Split data into training (80%) and testing (20%) sets\n",
    "- Using sklearn's train_test_split() instead of our manual function\n",
    "- Same random_state=113 ensures identical split to No-Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bde541d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 80,000 samples (80%)\n",
      "Test set size: 20,000 samples (20%)\n",
      "\n",
      "X_train shape: (80000, 11)\n",
      "X_test shape: (20000, 11)\n",
      "y_train shape: (80000,)\n",
      "y_test shape: (20000,)\n"
     ]
    }
   ],
   "source": [
    "# train_test_split() splits arrays into random train and test subsets\n",
    "\n",
    "# NO-FRAMEWORK COMPARISON\n",
    "# I manually wrote 15+ lines to:\n",
    "# - 1. Shuffle indices with np.random.shuffle()\n",
    "# - 2. Calculate split index\n",
    "# - 3. Slice arrays by indices\n",
    "# Scikit-learn does this in ONE line with optimized C code\n",
    "\n",
    "# Parameters:\n",
    "# - X : Feature matrix to split\n",
    "# - y : Target array to split\n",
    "# - test_size : Fraction for testing (0.2 = 20%)\n",
    "# - random_state : Seed for reproducibility (Important for fair comparison)\n",
    "# - shuffle = Whether to shuffle before splitting (default True)\n",
    "\n",
    "# Returns: X_train, X_test, y_train, y_test (in that order)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,                          # Features to split\n",
    "    y,                          # Target to split\n",
    "    test_size=0.2,              # 20% for testing, 80% for training\n",
    "    random_state=RANDOM_SEED    # Seed 113 for reporoducibility\n",
    ")\n",
    "\n",
    "# Verify split size match No-Framework\n",
    "# Training: 80k Samples\n",
    "# Testin: 20k Samples\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.0f}%)\")\n",
    "print(f\"Test set size: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.0f}%)\")\n",
    "\n",
    "print(f\"\\nX_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2478cc6",
   "metadata": {},
   "source": [
    "# Feature Scaling (Z-Score Normalization)\n",
    "\n",
    "- Standardize features to have mean=0 and std=1\n",
    "- Using sklearn's StandardScaler instead of our manual function\n",
    "- CRITICAL: Fit on training data only, then transofm both train and test (similar to NF)\n",
    "- This prevents data leakage (test data statistics leaking into training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbc830e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling parameters (computed from training data):\n",
      "\n",
      "Feature                    Mean             Std\n",
      "-----------------------------------------------\n",
      "year                    2012.32            5.79\n",
      "manufacturer              18.24           11.48\n",
      "condition                  3.09            2.43\n",
      "cylinders                  6.00            1.92\n",
      "fuel                       2.05            0.78\n",
      "odometer               94235.84        62977.76\n",
      "title_status               0.23            1.06\n",
      "transmission               0.39            0.77\n",
      "drive                      1.40            1.21\n",
      "type                       7.14            4.12\n",
      "state                     23.60           15.10\n",
      "\n",
      "--- Verification (Training Data After Scaling) ---\n",
      "Mean of each feature (should be 0): [ 0.  0.  0. -0.  0. -0. -0.  0.  0. -0. -0.]\n",
      "Std of each feature (should be 1): [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# StandardScaler performs z-score normalization: z = (x - mean) / std\n",
    "\n",
    "# NO-FRAMEWORK COMPARISON:\n",
    "# I manually wrote two functions:\n",
    "# 1. compute_scaling_params(): Calculate mean and std from training data\n",
    "# 2. scale_features(): Apply (x - mean) / std transofrmation\n",
    "# StandardScaler combines these with fit() and transofmr() methods\n",
    "\n",
    "# CRITICAL BEST PRACTICE:\n",
    "# - fit(): Only call on TRAINING DATA\n",
    "# - transform(): Apply to BOTH train and test (uses learned parameters)\n",
    "# - NEVER fit on test data - this causes data leakage\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit_transform() combines fit() and transform() in one step\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# transform() applies the SAME scaling (using mean/std from training)\n",
    "# This ensures test data is scaled consistently with training data\n",
    "# NEVER use fit_transoform() on test data - it would calculate new mean/std\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Display the learned scaling parameters\n",
    "print(\"Scaling parameters (computed from training data):\\n\")\n",
    "print(f\"{'Feature':<15} {'Mean':>15} {'Std':>15}\")\n",
    "print(\"-\" * 47)\n",
    "for i, name in enumerate(FEATURE_NAMES):\n",
    "    print(f\"{name:<15} {scaler.mean_[i]:>15.2f} {scaler.scale_[i]:>15.2f}\")\n",
    "\n",
    "# Verify scaling worked - training data should have mean=0 and std=1\n",
    "print(\"\\n--- Verification (Training Data After Scaling) ---\")\n",
    "print(f\"Mean of each feature (should be 0): {np.mean(X_train_scaled, axis=0).round(6)}\")\n",
    "print(f\"Std of each feature (should be 1): {np.std(X_train_scaled, axis=0).round(6)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6815d29",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "\n",
    "- Using sklearn's LinearRegression instead of our manual gradient descent\n",
    "- KEY DIFFERENCES: Sklearn uses the Normal equation (closed-form solution)\n",
    "- This calculates the optimal weights in ONE step (no iterations needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e871227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned bias (intercept): $ 19,208.98\n",
      "\n",
      "Learned weights (coefficients):\n",
      "    year           :    5226.18\n",
      "    manufacturer   :    -586.24\n",
      "    condition      :     679.35\n",
      "    cylinders      :    2429.28\n",
      "    fuel           :   -2696.31\n",
      "    odometer       :   -4391.57\n",
      "    title_status   :    -710.06\n",
      "    transmission   :    1351.93\n",
      "    drive          :   -2181.78\n",
      "    type           :      67.64\n",
      "    state          :       2.57\n"
     ]
    }
   ],
   "source": [
    "# LinearREgression uses the Normal Equation to find optimal weights\n",
    "\n",
    "# NO-FRAMEWORK COMPARISON:\n",
    "# I manually wrote:\n",
    "# - forward(): Calculate predictions\n",
    "# - compute_cost(): Calculate MSE\n",
    "# - compute_gradients(): Calculate dW and db\n",
    "# - train(): Loop through 1000 iterations updating weights\n",
    "# Sklearn does all this in one method call using linear algebra\n",
    "\n",
    "# Create a LinearRegression model instance\n",
    "# fit_intercept=True (default) means it will learn a bias term\n",
    "# This matches the No-framework implementation where we learned weights and bias\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "\n",
    "# fit() trains the model by finding optimal weights\n",
    "# Sklearn does this by:\n",
    "# 1. Adds a column of 1s to X for the intercept (bias)\n",
    "# 2. Solves the normal equation using optimized linear algebra\n",
    "# 3. Stores results in model.coef_ (weights) and model.intercept_ (bias)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Display the learned parameters\n",
    "# model.intercept_ is the bias term (single scalar)\n",
    "# NO-FRAMEWORK COMPARISON: This is equivalent to the learned 'bias' variable\n",
    "print(f\"Learned bias (intercept): $ {model.intercept_:,.2f}\")\n",
    "\n",
    "# model.coef_ contains the weights (one per feature)\n",
    "# NO-FRAMEWORK COMPARISON: This is equivalent to the learned 'weights' array\n",
    "print(\"\\nLearned weights (coefficients):\")\n",
    "for i, name in enumerate(FEATURE_NAMES):\n",
    "    print(f\"    {name:<15}: {model.coef_[i]:>10.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dd512d",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "- Evaluate model performance on both training and test sets\n",
    "- Using sklearn's mean_squared_error and r2_score instead of our manual functions\n",
    "- Same metrics as No-Framework: MSE, RMSE, R^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06af252e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "MODEL EVALUATION\n",
      "==================================================\n",
      "\n",
      "--- Training Set Performance ---\n",
      "MSE:    101,652,942.00\n",
      "RMSE:   $10,082.31\n",
      "R^2:    0.4933\n",
      "\n",
      "--- Test Set Performance ---\n",
      "MSE:    102,115,067.30\n",
      "RMSE:   $10,105.20\n",
      "R^2:    0.4986\n",
      "\n",
      " --- Interpretation ---\n",
      "On average, predictions are off by $10,105\n",
      "The model explains 49.9% of the variance in car prices\n",
      "Good generalization (R^2 gap: -0.005)\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on training data\n",
    "# model.predict() computes: X @ weights + bias\n",
    "y_train_pred = model.predict(X_train_scaled)\n",
    "\n",
    "# Make predictions on test data\n",
    "y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# TRAIN SET METRICS\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "\n",
    "# To get RMSE, I will use what we did in No-Framework\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "\n",
    "# r2_score() calcualtes r^2\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# TEST SET METRICS\n",
    "\n",
    "# Same calculations for test set\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Display results in same format as No-framework\n",
    "print(\"=\" * 50)\n",
    "print(\"MODEL EVALUATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n--- Training Set Performance ---\")\n",
    "print(f\"MSE:    {train_mse:,.2f}\")\n",
    "print(f\"RMSE:   ${train_rmse:,.2f}\")\n",
    "print(f\"R^2:    {train_r2:.4f}\")\n",
    "\n",
    "print(\"\\n--- Test Set Performance ---\")\n",
    "print(f\"MSE:    {test_mse:,.2F}\")\n",
    "print(f\"RMSE:   ${test_rmse:,.2f}\")\n",
    "print(f\"R^2:    {test_r2:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\n --- Interpretation ---\")\n",
    "print(f\"On average, predictions are off by ${test_rmse:,.0f}\")\n",
    "print(f\"The model explains {test_r2*100:.1f}% of the variance in car prices\")\n",
    "\n",
    "# Check for overfitting (gap between train and test performance)\n",
    "r2_gap = train_r2 - test_r2\n",
    "if r2_gap > 0.05:\n",
    "    print(f\"WARNING: Potential overfitting (R^2 gap: {r2_gap:.3f})\")\n",
    "else:\n",
    "    print(f\"Good generalization (R^2 gap: {r2_gap:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b090662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML Portfolio (.venv)",
   "language": "python",
   "name": "ml-portfolio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
