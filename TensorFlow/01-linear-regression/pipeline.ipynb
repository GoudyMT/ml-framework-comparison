{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da8eaf69",
   "metadata": {},
   "source": [
    "# TensorFlow Linear Regression\n",
    "\n",
    "### This implementation uses TensorFlow/Keras to build a linear regression model\n",
    "\n",
    "### Goal: Predict used car prices and compare with No-Framework, Scikit-Learn, and PyTorch\n",
    "\n",
    "What TensorFlow/Keras provides (that we built manually in No-Framework):\n",
    "- `tf.keras.Sequential`: High-level API for building models layer by layer\n",
    "- `tf.keras.layers.Dense`: Fully connected layer (replaces manual weights + bias)\n",
    "- `tf.keras.losses.MeanSquaredError`: Pre-built loss function\n",
    "- `tf.keras.optimizers.SGD`: Optimizer that handles parameter updates\n",
    "- `model.fit()`: Complete training loop in one line\n",
    "\n",
    "Key Concept - Keras vs Raw TensorFlow:\n",
    "- TensorFlow 2.x uses Keras as its high-level API\n",
    "- Keras abstracts away the computational graph complexity\n",
    "- Similar to PyTorch's nn.Module, but with even simpler syntax via Sequential API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e887299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Imports successful!\n",
      "TensorFlow version: 2.20.0\n",
      "Random seed set to: 113\n"
     ]
    }
   ],
   "source": [
    "# tensorflow: The main TensorFlow Library\n",
    "import tensorflow as tf\n",
    "\n",
    "# numpy: Still needed for initial data handling\n",
    "import numpy as np\n",
    "\n",
    "# pandas: For loading CSV data\n",
    "import pandas as pd\n",
    "\n",
    "# matplotlib: for visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# os: File path handling\n",
    "import os\n",
    "\n",
    "# Sklearn utilities: Using these for consistency with previous implementations\n",
    "# This ensures identical train/test splits and scaling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Performance tracking\n",
    "import time\n",
    "import tracemalloc\n",
    "import platform\n",
    "\n",
    "# Set random see for reproducibility\n",
    "RANDOM_SEED = 113\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "print(\"All Imports successful!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Random seed set to: {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3388ce",
   "metadata": {},
   "source": [
    "# Load Cleaned Data\n",
    "\n",
    "- Load the same pre-processed dataset used in N0-Framework, Scikit-Learn, and PyTorch\n",
    "- Using pandas for consistency with SL implementation\n",
    "- This ensures fair comparison across all frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84639ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (100000, 12)\n",
      "Columns: ['price', 'year', 'manufacturer', 'condition', 'cylinders', 'fuel', 'odometer', 'title_status', 'transmission', 'drive', 'type', 'state']\n",
      "\n",
      "First 3 rows:\n",
      "   price    year  manufacturer  condition  cylinders  fuel  odometer  \\\n",
      "0  29990  2014.0             7          2          6     2   26129.0   \n",
      "1   6995  2006.0            12          0          6     2  198947.0   \n",
      "2   4995  2009.0            35          6          8     2  152794.0   \n",
      "\n",
      "   title_status  transmission  drive  type  state  \n",
      "0             0             2      0     8     17  \n",
      "1             6             0      3    10      5  \n",
      "2             0             0      3    11     22  \n"
     ]
    }
   ],
   "source": [
    "# Define path to our cleaned dataset\n",
    "DATA_PATH = os.path.join('..', '..', 'data', 'processed', 'vehicles_clean.csv')\n",
    "\n",
    "# Load data using pandas\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Verify data loaded correctly\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608fb65e",
   "metadata": {},
   "source": [
    "# Separate Features and Target\n",
    "\n",
    "- Price is our TARGET variable \n",
    "- All other columns are FEATURES\n",
    "- Same seperation as Scikit-Learn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82fa72e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features (X) shape: (100000, 11)\n",
      "Target (y) shape: (100000,)\n",
      "\n",
      "Feature Names: ['year', 'manufacturer', 'condition', 'cylinders', 'fuel', 'odometer', 'title_status', 'transmission', 'drive', 'type', 'state']\n"
     ]
    }
   ],
   "source": [
    "# Define target and column variable (same as all previous implementations)\n",
    "TARGET_COLUMN = 'price'\n",
    "FEATURE_COLUMNS = [ 'year', 'manufacturer', 'condition', 'cylinders',\n",
    "                    'fuel', 'odometer', 'title_status', 'transmission', 'drive', 'type', 'state']\n",
    "\n",
    "# Extract target (y) and features (X) as numpy arrays\n",
    "y = df[TARGET_COLUMN].values\n",
    "X = df[FEATURE_COLUMNS].values\n",
    "\n",
    "# Store feature names for later use (displaying learned weights)\n",
    "FEATURE_NAMES = FEATURE_COLUMNS\n",
    "\n",
    "# Verify shapes \n",
    "print(f\"Features (X) shape: {X.shape}\")\n",
    "print(f\"Target (y) shape: {y.shape}\")\n",
    "print(f\"\\nFeature Names: {FEATURE_NAMES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92846e29",
   "metadata": {},
   "source": [
    "# Train/Test Split\n",
    "\n",
    "- Using Sklearn's train_test_split for consistency with Scikit-learn implementation\n",
    "- Same 80/20 split, same random seed (113)\n",
    "- This ensures we're comparing apples-to-applies across frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b60700dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 80,000 samples (80%)\n",
      "Test set size: 20,000 samples (20%)\n",
      "\n",
      "X_train shape: (80000, 11)\n",
      "X_test shape: (20000, 11)\n",
      "y_train shape: (80000,)\n",
      "y_test shape: (20000,)\n"
     ]
    }
   ],
   "source": [
    "# Split data using sklearn (same as Scikit-Learn and PyTorch Implementations)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,                          # Features to split\n",
    "    y,                          # Target to split\n",
    "    test_size=0.2,              # 20% for testing, 80% for training\n",
    "    random_state=RANDOM_SEED    # Seed 113 for reporoducibility\n",
    ")\n",
    "\n",
    "# Verify split sizes\n",
    "print(f\"Training set size: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.0f}%)\")\n",
    "print(f\"Test set size: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.0f}%)\")\n",
    "print(f\"\\nX_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb153df",
   "metadata": {},
   "source": [
    "# Feature Scaling (Z-Score Normalization)\n",
    "\n",
    "- Using sklearn's StandardScaler for consistency with Scikit-Learn implementation\n",
    "- Same z-score noramlization\n",
    "- Fit on training data only, transform both train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2efdbaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling parameters (computed from training data):\n",
      "\n",
      "Feature                    Mean             Std\n",
      "-----------------------------------------------\n",
      "year                    2012.32            5.79\n",
      "manufacturer              18.24           11.48\n",
      "condition                  3.09            2.43\n",
      "cylinders                  6.00            1.92\n",
      "fuel                       2.05            0.78\n",
      "odometer               94235.84        62977.76\n",
      "title_status               0.23            1.06\n",
      "transmission               0.39            0.77\n",
      "drive                      1.40            1.21\n",
      "type                       7.14            4.12\n",
      "state                     23.60           15.10\n",
      "\n",
      "--- Verification (Training Data After Scaling) ---\n",
      "Mean of each feature (should be 0): [ 0.  0.  0. -0.  0. -0. -0.  0.  0. -0. -0.]\n",
      "Std of each feature (should be 1): [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Create and fit scaler on training data (same as Scikit-Learn)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit_transform on training data: calculates mean/std AND applies scaling\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# transform on test data: uses mean/std from training (no data leakage)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Display the learned scaling parameters\n",
    "print(\"Scaling parameters (computed from training data):\\n\")\n",
    "print(f\"{'Feature':<15} {'Mean':>15} {'Std':>15}\")\n",
    "print(\"-\" * 47)\n",
    "for i, name in enumerate(FEATURE_NAMES):\n",
    "    print(f\"{name:<15} {scaler.mean_[i]:>15.2f} {scaler.scale_[i]:>15.2f}\")\n",
    "\n",
    "# Verify scaling worked - training data should have mean=0 and std=1\n",
    "print(\"\\n--- Verification (Training Data After Scaling) ---\")\n",
    "print(f\"Mean of each feature (should be 0): {np.mean(X_train_scaled, axis=0).round(6)}\")\n",
    "print(f\"Std of each feature (should be 1): {np.std(X_train_scaled, axis=0).round(6)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f494f1b2",
   "metadata": {},
   "source": [
    "# Start Performance Tracking\n",
    "\n",
    "- Begin measuring time and memory BEFORE model initialization\n",
    "- This captures model creation, compilation, and training\n",
    "- Matches the timing approachf used in No-Framework and PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfde4476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance tracking started...\n",
      " - Memory tracking: ACTIVE\n",
      " - Time: STARTED\n"
     ]
    }
   ],
   "source": [
    "# Start memory tracking\n",
    "tracemalloc.start()\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Performance tracking started...\")\n",
    "print(\" - Memory tracking: ACTIVE\")\n",
    "print(\" - Time: STARTED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b814eaa",
   "metadata": {},
   "source": [
    "# Define the Model\n",
    "\n",
    "TensorFlow uses Keras as its high-level API. For linear regression, we use:\n",
    "- `tf.keras.Sequential`: Container for stacking layers linearly\n",
    "- `tf.keras.layers.Dense`: Fully connected layer (like PyTorch's nn.Linear)\n",
    "\n",
    "| TensorFlow/Keras | PyTorch Equivalent | No-Framework Equivalent |\n",
    "|------------------|-------------------|------------------------|\n",
    "| `Sequential([Dense(1)])` | `nn.Linear(11, 1)` | `weights = np.zeros(11)` + `bias = 0` |\n",
    "| `model(X)` | `model(X)` | `X @ weights + bias` |\n",
    "| `model.get_weights()` | `model.parameters()` | Manual weight/bias variables |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80931cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m12\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12</span> (48.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m12\u001b[0m (48.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12</span> (48.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12\u001b[0m (48.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Weights shape: (11, 1)\n",
      "Initial Bias shape: (1,)\n",
      "\n",
      "Initial weights (first 5): [-0.27453867  0.49492913 -0.21581462 -0.04182166  0.44886726]\n",
      "Initial Bias: [0.]\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "\n",
    "# tf.keras.Sequential creates a model by stacking layers in order\n",
    "# For linear regression, we need just one Dense layer with 1 output\n",
    "\n",
    "# Dense layer parameters:\n",
    "# - units=1: One output\n",
    "# - input_shape=(11,): 11 input features\n",
    "# - Automatically creates weights (11,) and bias (1,)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(11,)),\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "# Displayu model architecture\n",
    "print(\"Model Architecture:\")\n",
    "model.summary()\n",
    "\n",
    "# View initial weights (randomly initialized by default)\n",
    "# get_weights() returns [weights_array, bias_array]\n",
    "weights, bias = model.get_weights()\n",
    "print(f\"\\nInitial Weights shape: {weights.shape}\")\n",
    "print(f\"Initial Bias shape: {bias.shape}\")\n",
    "print(f\"\\nInitial weights (first 5): {weights[:5, 0]}\")\n",
    "print(f\"Initial Bias: {bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f65c239",
   "metadata": {},
   "source": [
    "# Compile the Model\n",
    "\n",
    "TensorFlow requires a separate `compile()` step before training. This configures:\n",
    "- **Loss function**: What to minimize (MSE for regression)\n",
    "- **Optimizer**: How to minimize it (SGD with learning rate 0.01)\n",
    "\n",
    "| TensorFlow | PyTorch Equivalent |\n",
    "|------------|-------------------|\n",
    "| `model.compile(loss, optimizer)` | Separate `criterion` and `optimizer` objects |\n",
    "| Configured once before training | Used explicitly in training loop |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f068f49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiled successfully!\n",
      " Optimizer: SGD (learning_rate=0.01)\n",
      " Loss function: Mean Squared Error (MSE)\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "\n",
    "# mode.compile() configures the model for training\n",
    "# This is required BEFORE calling model.fit()\n",
    "\n",
    "# Parameters:\n",
    "# - Optimizer: SGD with learning_rate=0.01\n",
    "# - loss: Mean Squared Error\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "    loss='mse'\n",
    ")\n",
    "\n",
    "print(\"Model compiled successfully!\")\n",
    "print(\" Optimizer: SGD (learning_rate=0.01)\")\n",
    "print(\" Loss function: Mean Squared Error (MSE)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a17b231",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "\n",
    "TensorFlow's `model.fit()` handles the ENTIRE training loop in one line:\n",
    "- Forward pass\n",
    "- Loss computation\n",
    "- Backward pass (gradient computation)\n",
    "- Weight updates\n",
    "\n",
    "| TensorFlow | PyTorch Equivalent | No-Framework Equivalent |\n",
    "|------------|-------------------|------------------------|\n",
    "| `model.fit(X, y, epochs=1000)` | Manual for loop (1000 iterations) | Manual for loop (1000 iterations) |\n",
    "| ~1 line | ~15 lines | ~30 lines |\n",
    "\n",
    "The `history` object returned by `fit()` contains loss values for plotting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f87a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Linear Regression Model...\n",
      "==================================================\n",
      "==================================================\n",
      "Training Complete!\n",
      "  Initial Loss: 569,614,528.00\n",
      "  Final Loss:   101,652,944.00\n",
      "\n",
      "--- Training Performance ---\n",
      "  Training time: 24.6862 seconds\n",
      "  Peak memory:   8.38 MB\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "# model.fit() performs the entire training loop:\n",
    "#   1. Forward pass (predictions)\n",
    "#   2. Loss computation (MSE)\n",
    "#   3. Backward pass (gradients via automatic differentiation)\n",
    "#   4. Weight updates (SGD optimizer step)\n",
    "# All in one line of code\n",
    "\n",
    "# Parameters:\n",
    "#   - X_train_scaled: Input features (scaled)\n",
    "#   - y_train: Target values\n",
    "#   - epochs: Numer of training iterations (1000, same as previous frameworks)\n",
    "#   - batch_size: Use full dataset per epoch (same as previous frameworks)\n",
    "#   - verbose: 0=silent, 1=progress bar, 2= one line per epoch\n",
    "#   - shuffle: False to match our other implementations\n",
    "\n",
    "print(\"Training Linear Regression Model...\")\n",
    "\n",
    "# Train the model - this replaces the entire manual training loop\n",
    "history = model.fit(\n",
    "    X_train_scaled,         # Input features\n",
    "    y_train,                # Target values\n",
    "    epochs=1000,            # Same as previous frameworks\n",
    "    batch_size=len(X_train),# Full batch per epoch\n",
    "    verbose=0,              # Silent training - printing summary after\n",
    "    shuffle=False           # Don't shuffle to match other frameworks\n",
    ")\n",
    "\n",
    "# STOP Performance tracking\n",
    "end_time = time.time()\n",
    "current_mem, peak_mem = tracemalloc.get_traced_memory()\n",
    "\n",
    "# Calculate performance metrics\n",
    "training_time = end_time - start_time\n",
    "peak_memory_mb = peak_mem / 1024 / 1024\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Training Complete!\")\n",
    "print(f\"  Initial Loss: {history.history['loss'][0]:,.2f}\")\n",
    "print(f\"  Final Loss:   {history.history['loss'][-1]:,.2f}\")\n",
    "print(f\"\\n--- Training Performance ---\")\n",
    "print(f\"  Training time: {training_time:.4f} seconds\")\n",
    "print(f\"  Peak memory:   {peak_memory_mb:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML Portfolio (.venv)",
   "language": "python",
   "name": "ml-portfolio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
