{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5143572",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN) - TensorFlow Implementation\n",
    "\n",
    "Multi-class classification on the **Covertype (Forest Cover Type)** dataset using TensorFlow tensor operations.\n",
    "\n",
    "**Dataset**: 581,012 samples, 54 features, 7 forest cover types  \n",
    "**Task**: Predict forest cover type from cartographic variables  \n",
    "**Key Concept**: KNN is a \"lazy learner\" - no training phase, expensive at prediction time\n",
    "\n",
    "## TensorFlow Approach for KNN\n",
    "- **Tensor operations**: Broadcasting for pairwise distance computation\n",
    "- **`tf.math.top_k`**: Efficient K-nearest selection\n",
    "- **Batched processing**: Memory management for large datasets\n",
    "\n",
    "## Important Note: CPU-Only Execution\n",
    "- TensorFlow 2.11+ dropped native Windows GPU support. This implementation runs on CPU.\n",
    "- For GPU acceleration on Windows, options include WSL2 or TensorFlow 2.10 with Python â‰¤3.10.\n",
    "- GPU setup will be configured when we reach neural network models (DNNs, CNNs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f09b331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "Running on CPU (TF 2.11+ dropped native Windows GPU support)\n",
      "Imports complete!\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# TensorFlow for GPU-accelerated tensor operations\n",
    "import tensorflow as tf\n",
    "\n",
    "# Add utils to path\n",
    "sys.path.append('../..')\n",
    "from utils.data_loader import load_processed_data\n",
    "from utils.metrics import accuracy, macro_f1_score\n",
    "from utils.visualization import (\n",
    "    plot_confusion_matrix_multiclass,\n",
    "    plot_per_class_f1\n",
    ")\n",
    "from utils.performance import track_performance\n",
    "\n",
    "# Check device\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"GPU available: {gpus[0].name}\")\n",
    "else:\n",
    "    print(\"Running on CPU (TF 2.11+ dropped native Windows GPU support)\")\n",
    "\n",
    "print(\"Imports complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d79fcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 464,809 samples, 54 features\n",
      "Test set: 116,203 samples\n",
      "Classes (7): ['Spruce/Fir', 'Lodgepole Pine', 'Ponderosa Pine', 'Cottonwood/Willow', 'Aspen', 'Douglas-fir', 'Krummholz']\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed Data\n",
    "\"\"\"\n",
    "Load the same Covertype dataset used by Scikit-learn, No-Framework, and PyTorch.\n",
    "This ensures fair comparison across all 4 frameworks.\n",
    "\"\"\"\n",
    "\n",
    "X_train, X_test, y_train, y_test, metadata = load_processed_data('knn')\n",
    "\n",
    "# Extract metadata for reference\n",
    "class_names = metadata['class_names']\n",
    "n_classes = metadata['n_classes']\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples, {X_train.shape[1]} features\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
    "print(f\"Classes ({n_classes}): {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ab95175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train tensor: (464809, 54), dtype=<dtype: 'float32'>\n",
      "X_test tensor:  (116203, 54), dtype=<dtype: 'float32'>\n",
      "Device: /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "# Convert Data to TensorFlow Tensors\n",
    "\"\"\"\n",
    "Convert NumPy arrays to TensorFlow constant tensors.\n",
    "Unlike pytorch, tensorflow tensors are immutable by default (tf.constant).\n",
    "Running on CPU since TF 2.11+ dropped native windows gpu support.\n",
    "\"\"\"\n",
    "\n",
    "# Convert to tensorflow tensors\n",
    "# tf.constant creates immutable tensors (vs pytorchs mutable torch.tensor)\n",
    "X_train_t = tf.constant(X_train, dtype=tf.float32)\n",
    "X_test_t = tf.constant(X_test, dtype=tf.float32)\n",
    "y_train_t = tf.constant(y_train, dtype=tf.int64)\n",
    "y_test_t = tf.constant(y_test, dtype=tf.int64)\n",
    "\n",
    "print(f\"X_train tensor: {X_train_t.shape}, dtype={X_train_t.dtype}\")\n",
    "print(f\"X_test tensor:  {X_test_t.shape}, dtype={X_test_t.dtype}\")\n",
    "print(f\"Device: {X_train_t.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66d2c6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN prediction function defined (TensorFlow)\n",
      "Using: K=3, Manhattan distance, distance-weighted voting\n",
      "Chunked distance computation (train_chunk_size=5000, batch_size=500)\n"
     ]
    }
   ],
   "source": [
    "# KNN Prediction Function (TensorFlow)\n",
    "\"\"\"\n",
    "TensorFlow KNN using tensor operations for distance computation.\n",
    "\n",
    "Key differences from PyTorch:\n",
    "    - No tf.cdist equivalent - use broadcasting for Manhattan distance\n",
    "    - tf.math.top_k returns LARGEST by default (need to negate distances)\n",
    "    - TensorFlow tensors are immutable - different memory patterns\n",
    "    - Running on CPU (TF 2.11+ dropped native Windows GPU support)\n",
    "    - Must chunk along training axis to avoid OOM (no optimized kernel like torch.cdist)\n",
    "\n",
    "Memory consideration:\n",
    "    Broadcasting creates a 3D intermediate: (batch_size, n_train, n_features)\n",
    "    With full X_train: 500 x 464,809 x 54 x 4 bytes = ~50 GB (will crash!)\n",
    "    Chunked: 500 x 5,000 x 54 x 4 bytes = ~540 MB per chunk (manageable)\n",
    "\n",
    "Best hyperparameters (from Scikit-Learn GridSearchCV):\n",
    "    - K=3, Manhattan distance (p=1), distance-weighted voting\n",
    "\"\"\"\n",
    "\n",
    "def manhattan_distance_chunked(batch, X_train, train_chunk_size=5000):\n",
    "    \"\"\"\n",
    "    Memory-efficient manhattan distance by chunking along the training axis.\n",
    "\n",
    "    Without chunking, broadcasting creates a (batch_size, 464809, 54) tensor which requires ~50-200 GB depending on batch_size.\n",
    "    By processing X_train in chunks, peak memory stays under ~540 MB per chunk.\n",
    "\n",
    "    Args:\n",
    "        batch: test samples (batch_size, n_features)\n",
    "        X_train: training samples (n_train, n_features)\n",
    "        train_chunk_size: number of training samples per chunk\n",
    "    \n",
    "    Returns:\n",
    "        distance: (batch_size, n_train) manhattan distances\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    for c_start in range(0, X_train.shape[0], train_chunk_size):\n",
    "        c_end = min(c_start + train_chunk_size, X_train.shape[0])\n",
    "        chunk = X_train[c_start:c_end]\n",
    "        # Intermediate: (batch_size, chunk_size, 54) - much smaller\n",
    "        diff = tf.abs(tf.expand_dims(batch, 1) - tf.expand_dims(chunk, 0))\n",
    "        chunks.append(tf.reduce_sum(diff, axis=2))\n",
    "    return tf.concat(chunks, axis=1)\n",
    "\n",
    "def knn_predict_tf(X_train, y_train, X_test, k=3, batch_size=500, n_classes=7):\n",
    "    \"\"\"\n",
    "    TensorFlow KNN prediction with manhattan distance and distance weighting.\n",
    "\n",
    "    Args:\n",
    "        - X_train: training features tensor (n_train, n_features)\n",
    "        - y_train: training labels tensor (n_train,)\n",
    "        - X_test: Test features tensor (n_test, n_features)\n",
    "        - k: number of neighbors\n",
    "        - batch_size: samples per batch (manages memory)\n",
    "        - n_classes: number of classes for voting\n",
    "    \n",
    "    Returns:\n",
    "        predictions: predicted class labels (n_test,)\n",
    "    \"\"\"\n",
    "    n_test = X_test.shape[0]\n",
    "    all_predictions = []\n",
    "\n",
    "    for start_idx in range(0, n_test, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, n_test)\n",
    "        batch = X_test[start_idx:end_idx]\n",
    "\n",
    "        # Compute manhattan (L1) distances using chunked approahc\n",
    "        distances = manhattan_distance_chunked(batch, X_train)\n",
    "\n",
    "        # Find k nearest neighbors\n",
    "        # tf.math.top_k returns largest values, so negate distances to get smallest\n",
    "        neg_distances = tf.negative(distances)\n",
    "        k_values, k_indices = tf.math.top_k(neg_distances, k=k)\n",
    "        k_distances = tf.negative(k_values) # Convert back to positive distances\n",
    "\n",
    "        # Get labels of K nearest neighbors\n",
    "        k_labels = tf.gather(y_train, k_indices)    # shape: (batch_size, k)\n",
    "\n",
    "        # Distance-weighted voting: weight = 1 / (distance + epsilon)\n",
    "        epsilon = 1e-8\n",
    "        weights = 1.0 / (k_distances + epsilon) # shape: (batch_size, k)\n",
    "\n",
    "        # Accumulate weighted votes for each class\n",
    "        # Labels are 1-indexed (1-7), subtract 1 for array indexing (0-6)\n",
    "        batch_preds = []\n",
    "        for i in range(end_idx - start_idx):\n",
    "            class_weights = tf.zeros(n_classes)\n",
    "            for j in range(k):\n",
    "                class_idx = k_labels[i, j] - 1 \n",
    "                # tf.tensor_scatter_nd_update to add weight at class index\n",
    "                class_weights = class_weights + tf.one_hot(class_idx, n_classes) * weights[i, j]\n",
    "            batch_preds.append(tf.argmax(class_weights) + 1)\n",
    "        \n",
    "        all_predictions.extend(batch_preds)\n",
    "    \n",
    "    return tf.cast(tf.stack(all_predictions), dtype=tf.int64)\n",
    "\n",
    "print(\"KNN prediction function defined (TensorFlow)\")\n",
    "print(f\"Using: K=3, Manhattan distance, distance-weighted voting\")\n",
    "print(f\"Chunked distance computation (train_chunk_size=5000, batch_size=500)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edc12a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Quick test to verify correctness on small subset\\n# Test on ~5% of test set before commiting to full run.\\n# This should take 5-10 minutes on cpu.\\n\\n\\ntest_size = len(X_test_t) // 20  # ~5,810 samples\\nprint(f\"Quick test: {test_size:,} samples (5% of test set)\")\\nprint(\"-\" * 40)\\n\\nwith track_performance() as perf:\\n    y_pred_small = knn_predict_tf(X_train_t, y_train_t, X_test_t[:test_size], k=3, batch_size=500, n_classes=n_classes)\\n\\n# Check accuracy on subset\\ny_pred_small_np = y_pred_small.numpy()\\nsmall_acc = accuracy(y_test[:test_size], y_pred_small_np)\\n\\nprint(f\"\\nTime: {perf[\\'time\\']:.2f} seconds\")\\nprint(f\"Peak memory: {perf[\\'memory\\']:.2f} MB\")\\nprint(f\"Accuracy on subset: {small_acc:.4f}\")\\nprint(f\"Throughput: {test_size / perf[\\'time\\']:,.0f} samples/second\")\\nprint(f\"\\nEstimated full run: {perf[\\'time\\'] * 20 / 60:.0f} minutes\")'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Quick test to verify correctness on small subset\n",
    "# Test on ~5% of test set before commiting to full run.\n",
    "# This should take 5-10 minutes on cpu.\n",
    "\n",
    "\n",
    "test_size = len(X_test_t) // 20  # ~5,810 samples\n",
    "print(f\"Quick test: {test_size:,} samples (5% of test set)\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "with track_performance() as perf:\n",
    "    y_pred_small = knn_predict_tf(X_train_t, y_train_t, X_test_t[:test_size], k=3, batch_size=500, n_classes=n_classes)\n",
    "\n",
    "# Check accuracy on subset\n",
    "y_pred_small_np = y_pred_small.numpy()\n",
    "small_acc = accuracy(y_test[:test_size], y_pred_small_np)\n",
    "\n",
    "print(f\"\\nTime: {perf['time']:.2f} seconds\")\n",
    "print(f\"Peak memory: {perf['memory']:.2f} MB\")\n",
    "print(f\"Accuracy on subset: {small_acc:.4f}\")\n",
    "print(f\"Throughput: {test_size / perf['time']:,.0f} samples/second\")\n",
    "print(f\"\\nEstimated full run: {perf['time'] * 20 / 60:.0f} minutes\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99cfecd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running KNN prediction on test set (CPU)...\n",
      "Test samples: 116,203\n",
      "Training samples: 464,809\n",
      "Batch size: 500, Train chunk size: 5000\n",
      "----------------------------------------\n",
      "\n",
      "Prediction time: 1055.34 seconds\n",
      "Peak memory: 24.30 MB\n",
      "Throughput: 110 samples/second\n"
     ]
    }
   ],
   "source": [
    "# Run KNN prediction with performance tracking\n",
    "\"\"\"\n",
    "Run tensorflow knn prediction on the full test set (CPU).\n",
    "\n",
    "Note: This will be significantly slower than pytorch gpu due to:\n",
    "    1. cpu-only execution\n",
    "    2. no tf.cdist equivalent (chunked broadcasting vs optimized cuda kernel)\n",
    "    3. python-level loops for weighted voting\n",
    "\"\"\"\n",
    "\n",
    "# Run prediction with performance tracking\n",
    "print(\"Running KNN prediction on test set (CPU)...\")\n",
    "print(f\"Test samples: {X_test_t.shape[0]:,}\")\n",
    "print(f\"Training samples: {X_train_t.shape[0]:,}\")\n",
    "print(f\"Batch size: 500, Train chunk size: 5000\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "with track_performance() as perf:\n",
    "    y_pred_t = knn_predict_tf(X_train_t, y_train_t, X_test_t, k=3, batch_size=500, n_classes=n_classes)\n",
    "\n",
    "# Convert predictions to NumPy for metrics\n",
    "y_pred = y_pred_t.numpy()\n",
    "\n",
    "print(f\"\\nPrediction time: {perf['time']:.2f} seconds\")\n",
    "print(f\"Peak memory: {perf['memory']:.2f} MB\")\n",
    "print(f\"Throughput: {len(X_test) / perf['time']:,.0f} samples/second\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML Portfolio (.venv)",
   "language": "python",
   "name": "ml-portfolio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
